% DEFINIR EL PROBLEMA

\section{Definición del problema}
% IDEA: me gustaria que esta sección  no se llamara DEFINICION DE PROBLEMA, intrinsicamente he hablado que el problema es como mejorar las predicciones secienciales usando lo mejor de dos áreas.



% esto creo que se repite en contexto o intro.
El problema de realizar modelos predictivos que minimicen su conjunto de entrenamientos, ha surgido hace años y diversos investigadores han trabajado con distintos enfoques. Los principales Rissanen\cite{Rissanen1983} y Langdom\cite{Langdon1983} en los laboratorios \emph{Bell}, al realizar pruebas y experimentar con un \emph{robot} que tiraba una moneda compitiendo con una persona, aquel robot realizaba todos los cálculos {markovianos} y las probabilidades condicionales para que cierto evento ocurra, a diferencia del sujeto que sólo estaba esperando un resultado aleatorio, la diferencia se marco en los costos de tiempo y de computo que se realizaron, por un parte la demora del \emph{robot} haciendo sus cálculos no mejoraba a la probabilidad aleatoria con que la persona a que enfrentaba 

Predecir no es trivial y requiere de gran cantidad información para poder realizar un modelo que logre abarcar varios escenarios reales, pero sí podemos llegar a acercarnos y minimizar el error  estaremos más cerca a predicciones exactas. Sin embargo, dos áreas han tratado de resolver el problema;  \losslessdatacompression~(\LDC) y \machinelearning de manera separada. Por parte de \LDC los mayores problemas son que los algoritmos predictivos  funcionan totalmente desconectados de la fuente de datos de entrada, esto implica que la validez del modelo solo es factible cuando esta realizando predicciones sin usuarios concurrentes o como se ha señalado anteriormente con un modelo con una componente \offline, lo que inhabilita rápidamente al modelo y en general no dan un resultado inmediato, en cambios en el esquema de \machinelearning debemos crear un modelo para entrenar y luego poder generar una función predictiva a lo cual se le suma un gran cantidad de datos para lograr un buen entrenamiento que produce un modelo bastante pesado para poder funcionar como modelo predictivo \online. 

Nuestro problema se acota a resolver predicciones secuenciales discretas con un modelo generado por un algoritmo de compresión, aplicado a un conjunto de datos discretos generados sintéticamente y otros real provistos en \emph{MSNBC}\cite{Claude2014}. 

Teniendo en funcionamiento un modelo de componentes híbrido juntando algoritmos y arquitectura de cada área para disponerlo como un servicio inmediato, es decir crear la componente \online del modelo predictivo con ayuda de \losslessdatacompression y el famoso algoritmo \texttt{LZ78}; dando una predictibilidad inmediata que hoy en la industria es necesaria para usar los datos recolectados y entregar nuevos enfoques a las decisiones basadas en esta perspectiva predictiva, como también dando un avance en el análisis de datos predictivos. 
Usando un algoritmo de tipo \LDC reemplazando en el proceso de aprendizaje de una arquitectura de  servicios  \machinelearning podemos alcanzar a una buena solución o a lo menos estar sobre el promedio aleatorio de ocurrencia de eventos.

