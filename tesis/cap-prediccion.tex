\chapter[Predicciones sobre Web Access]{Predicciones sobre Web Access} 
\label{ch:predicciones}

%Predicting the future is mostly a matter of managing not to blink as you witness the present.
%William Gibson

% Predecir es una cosa. Predecir correctamente es otra.

%%%%%%%Gueniche_Fournier-Viger_Raman_Tseng
%Moreover, machine learning algorithms such as neural networks and sequential rule mining have been applied to perform sequence prediction [6, 11].
Dentro del areá de las ciencias de computación, predecir es uno de los ultimos puntos que un roadmap de analisis se puede dar.
%https://dl.dropboxusercontent.com/spa/srrxi5k8b49mdt9/zn67b9c4.png
%https://www.safaribooksonline.com/library/view/introducing-data-science/9781633430037/kindle_split_010.html


Sobre temas anteriores, hemos hablado que una de las principales tareas de machine learning es hacer predicciones, es decir dado un numero x de entrenamiento, el sistema pude ser capaz de generalizar una entrada para lograr una predicción



 
Hemos explicado en los capitulos anteriores, variadas herramientas y conceptos claves para llegar a este capitulo, que comprende el objetivo de esta tesis.


Las predicciones es un proceso natural en que de cierta manera tomamos alguan decisión bastante rápido, inclusive uno no se da cuenta de esta decisión. Este desafio de anticipar la decisión es una la realización de análisis predictivos.


Las ventajas en varios campos de saber lo que pasará en un tiempo mas adelante, no da cierto un gran ventaja comparativa frente a competencias de mercado o en sí en cualquier escenario que podamos imaginar.


Que se requiere para hacer un realizar predicción, en un contexto simple ?

- Describir el escenario, en el que se harán las predicciones.
- Recolección de Datos
- Modelo que represente los datos
- Un algoritmo
- Metodlogía / Herramientas
- Una implementación eficiente


¿Que pasa cuando tenemos datos que queremos predecir y no estan ordenados?

- Los datos no están listos para ser procesados. CLAUDE logro hacer ya este trabajo para lograr una estructura eficietne dentro de webaccess log, ese problema no lo abordaremos.





``El modelo'', 

La realización de un modelo nunca es fácil, existe todo un proceso, ya se ha explicado en secciones anteriores por lo cual no vale la pena volver a mencionarlo.

De todas maneras, existen lineamientos que se pueden seguir:

- Recolección de información
- Entender la conceptualización de cualquier modelo presente.
- Conocer la naturaleza de la inforamción.
- Conocer que errores pueden ser falsos positvos, etc...
- Analisis del dominio 



``Errores'',

Nada es perfecto, por lo mismo un modeo predictivo es muy propenso a fallar, es importante saber que es lo que falla y ¿porque ?

Los errores pueden venir de cualquier áree

Las predicciones en cualquier dominio, entre mayor sea el umbral de distancia que queramos conocer que pasará, mayor son la cantidad de variables que vamos a tener que contemplar.



No model is perfect. In fact, any model even advertised as approaching perfection should be viewed with significant skepticism. It really is true with predictive analytics and modeling that if it looks too good to be true it probably is; there is almost certainly something very wrong with the sample, the analysis, or both. Errors can come from many areas; however, the following are a few common pitfalls.


``Cosas a tener en cuenta'' 

- Generalizar vs Precisar
- Valores anómalos
- No salir de la linea base que aborda el modelo
- Sobre-ajuste o overfitting del modelo




``Evaluar''

Siempre un modelo predictivo debe estar propenso a ser continuamente evaluado.




``Resultados''

Cualquier resultado que nos entregue un modelo predictivo debe ser consitente con lo que ha sido entrenado, un ejemplo burdo:  Es realizar un modelo predictivo para saber que clima habrá en una determinada hora y el resultado del modelo predictivo retorne un valor representado a una juego de azar.(malo el ejemplo)

\emph{La interpretación del resultado es clave}



\subsubsection{Herramientas para crear Modelos predictivos}



La clasificacion es un proceso que consta de dos pasos fundamentales. En el primer paso es donde se construye un modelo que describe un conjunto predeterminado de datos, clases o conceptos. Este modelo se construye tupla a tupla, describiendo en cada caso los atributos de la base de datos que se analiza. Se asume que cada tupla pertenece a una clase predefinida, que esta determinada por uno de los atributos llamado atributo clase. Las tuplas dentro de la clasificacion son tambien conocidas como ejemplos, instancias o experiencias (de aqui en adelante ejemplos). El conjunto de ejemplos analizados para construir el modelo forman el conjunto de datos de entrenamiento. Los ejemplos individuales que forman el conjunto de entrenamiento son seleccionados aleatoriamente de la poblacion muestral asignandole a cada uno, una etiqueta de clase, teniendo lugar el aprendizaje supervisado (se indica a que clase pertenece cada experiencia) ya mencionado anteriormente. Contrario al aprendizaje supervisado, el aprendizaje no supervisado, es aquel en el que no se conocen las etiquetas de clase de las experiencias, y el numero o conjunto de clases no se tienen a priori. Generalmente el modelo obtenido esta representado en la forma de reglas de clasificacion, arboles de decision o formulas matematicas. En el segundo paso, el modelo obtenido en el paso anterior es usado para prediccion y se puede estimar la precision de la prediccion del modelo (o clasificador) [17].
Segun Ye [29], a manera de resumen podemos decir que la clasificacion tiene como objetivo la construccion de modelos concisos que representen la distribucion
del atributo dependiente (clase) en funcion de los atributos predictores (atributos). El modelo resultante se usara, principalmente, para determinar la clase a la que pertenecen las observaciones de las que se conocen todos los valores de sus atributos con excepcion del valor de clase, es decir, su tarea sera preferentemente predictiva. Dependiendo del modelo generado, este ademas puede presentar caracteristicas descriptivas, lo que lo hara aun mas deseable desde el punto de vista de la extraccion de conocimiento.

La prediccion puede ser vista como la construccion y uso de un modelo para evaluar la clase de un objeto sin etiqueta, el valor o estimar los rangos que puede tener un objeto o atributo dado. La clasificacion y la regresion son los dos tipos principales de problemas de prediccion donde la clasificacion se usa para predecir valores discretos o nominales, mientras la regresion se usa para predecir valores continuos u ordenados. Sin embargo en este caso se puede referir al uso de prediccion para predecir etiquetas de clase, como clasificacion, y el uso de prediccion para predecir valores continuos (usando tecnicas de regresion) como prediccion, este punto de vista es el mas aceptado dentro de la Mineria de Datos [17]. La prediccion tiene aplicaciones numerosas dentro de las cuales podemos encontrar la aprobacion de credito, la prediccion de diagnostico medico, prediccion de desempen~o, y mercadeo selectivo.




Lo dificil de escoger un modelo de compresión es realmente si este funcionará con los datos que tengo.





Comprender las predicciones en los patrones de navegación \emph{web} es el problema principal, dado los nuevos sistemas es cada vez más común interactuar con \emph{web} inteligente. Estos sistemas deben tener cualidades que permitan adaptarse y tomar decisiones inmediatas en las situaciones que requiera el usuario. Aún mas necesaria es esta propiedad en problemas que requieren predicciones secuenciales, es decir, dada una secuencia de eventos  predecir el siguiente evento basado en nuestro limitado conocimiento histórico del caso estudio.

La compresión en este punto demuestra una convergencia en la predicción de secuencias discreta, debido a que existe un brecha en la compresión de los datos que comparte un espacio vectorial común, en el cual es posible ser un  algoritmo de compresión y ser candidato a generar un modelo predictivo. Usando toda la eficiencia de un algoritmo de compresión, es posible ofrecer toda una nueva perspectiva al ámbito de  predicciones que abarca los algoritmos tradicionales de \emph{Machine Learning}. 

%@TODO Esto podria hablar mas de webaccess
En los últimos años se ha observado que  los contenido de los sitios \emph{web} son dinámicos y nuevas páginas también se añaden a variados sitios de forma dinámica. Así es como  se hace necesario un modelo predictivo que pueda servir como un modelo predictivo \emph{online},  considerando tanto los cambios \emph{web} que se van produciendo, como también el comportamiento de los usuarios que interactúan con ella. 

Moghaddam \etal~\cite{Moghaddam2009} proponen un modelo predictivo \emph{online} que cubre la eficiencia de la memoria como un factor importante para un algoritmo \emph{online}. 

Para cualquier secuencia de eventos es posible modelar un proceso estocástico, algoritmos que emplean modelos de Markov para optimizar las predicciones del siguiente símbolo, en cualquier secuencia representativa  de sesión de navegación. Otros escenarios requieren que un algoritmo de predicción sea capaz de incrementar su recuperación de información  y poder dar resultados de forma inmediata, es decir predicciones $online$ .

El problema de la predicción secuencial se puede establecer de la siguiente manera, dado un secuencia de símbolos $ X = x_{1}, x_{2}, \dots, x_{n}\ $, ¿Cuál es el siguiente símbolo $x_{i+1}$? Este problema ha sido formulado gracias al trabajo de  Rissanen\cite{Rissanen1983} y Langdom\cite{Langdon1983}. Un buen compresor implementado en el ámbito de la predicción es aquel que al comprimir datos sin pérdida se aproxima a un \emph{predictor}.




%HACER UNA CITA CON ESTE AUTOR

\begin{verbatim}
 Our algorithm is based on LZ78 
 and LZW algorithms that are adapted for 
 modeling the user navigation in web. 
 Our model decreases computational 
 complexities which is 
 a serious problem in developing 
 online prediction systems. 

 A performance evaluation 
 is presented using real web logs. 
 This evaluation shows that our model 
 needs much less memory than PPM family 
 of algorithms with good prediction accuracy.
 
 In this paper we present efficient 
 techniques for modeling user navigation behavior. 
 Our model is online 
 so changes in user request patterns 
 will update our prediction model incrementally. 
 We do not build per-user predictive models. 
\end{verbatim}
\LDCMoghaddam


 % REVISAR ESTOS EJEMPLOS
 % https://www.safaribooksonline.com/library/view/predictive-analytics-revised/9781119145677/f04.xhtml



% TODO:
% Para mezclar con LZ78 existen harta informacion en la intro de  Begleiter2004

% ~cite{Begleiter2004}
% El componente de predicción de este algoritmo se discutió primero por Langdon (1983) y Rissanen (1983). La presentación de este algoritmo se simplifica después de que el bien conocido algoritmo de compresión LZ78, el cual funciona de la siguiente, se entiende. Dada una secuencia de Q1n ∈ Σn, LZ78 incrementalmente analiza Q1n en 'frases' adyacentes que no se solapan, que se recogen en un "diccionario" frase. El algoritmo comienza con un diccionario que contiene el ε frase vacía. En cada paso del algoritmo analiza una nueva frase, que es la frase más corta que todavía no está en el diccionario. Claramente, la frase s 'recién analizado se extiende una ya existente
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Also, it has been shown that a predictor with an order that grows at a rate approximating the entropy rate of the source is an optimal predictor. Another motivation to look to the field of text compression is that such algorithms are essentially incremental parsing algorithms, which is an extremely desirable quality in our search for an online predictor. Active LeZi is a predictor addressing this prediction problem, and is based on the above-mentioned motivations.


%CITED Gueniche_Fournier-Viger_Raman_Tseng
%Sequence prediction is an important task with many applications [1, 11]. Let be an alphabet of items (symbols) Z = {e1, e2, ..., em}. A sequence s is an ordered list of items s = ⟨i1,i2,...in⟩, where ik ∈ Z (1 ≤ k ≤ n). A prediction model is trained with a set of training sequences. Once trained, the model is used to perform sequence predictions. A prediction consists in predicting the next items of a sequence. This task has numerous applications such as web page prefetching, consumer product recommendation, weather forecasting and stock market prediction [1, 3, 8, 11].


%% ¿Por que predicciones en web access?




% Un string S = S1,n = S[1, n] = s1, s2, . . . , s es una secuencia de s ımbolos, donde cada s ımbolo pertenece al alfabeto Σ de taman ̃o σ. Un substring de S se escribe Si,j = S[i,j] = sisi+1...sj Un prefijo de S es un substring de la forma S1,j y un sufijo es un substring de la forma Si,n. Si i > j entonces Si,j = ε, el string vac ıo, de largo |ε| = 0. Un texto T = T1,n es un string terminado con un s ımbolo especial tn = $ ̸∈ Σ, lexicogr aficamente menor que cualquier otro s ımbolo en Σ. El orden lexicogr afico (<) entre strings se define como aX < bY si a < b ∨ (a = b ∧ X < Y ), donde a, b son s ımbolos y X, Y son strings sobre Σ.

Para poder crear un modelo predictivo acorde a la Teoría de la Información, un predictor que construye un modelo cuya entropía se aproxima a la de la fuente de datos, consigue una mayor exactitud predictiva. 


Las predicciones son un área importante dentro del dominio de \emph{Machine Learning} y la Inteligencia Artificial, las cuales pueden ofrecer un sistema de inteligencia que las aplicaciones necesitan para un óptimo desempeño, también ayudan a dar información para la toma de decisiones. Ciertos dominios requieren que la predicción se pueda realizar en las secuencias de eventos que, por lo general, se pueden modelar como un proceso estocástico. 
%Nuestro interés se centra en las predicciones de secuencias discretas, y en este punto, demostrar la convergencia en la  cual un modelo de compresión 
%(como el caso de \texttt{LZ78} que se explicará más detalladamente en el Capítulo 4) y la eficiencia de un algoritmo de compresión, ofrece una nueva perspectiva a las predicciones. 


% @TODO: hacer cita
% ACTIVE LEZI: AN INCREMENTAL PARSING ALGORITHM FOR SEQUENTIAL PREDICTION

En el caso  que un usuario realice una visita a un sitio web, el tiempo en el que pasa es registrado por el servidor. En esta investigación no se requiere indagar en temas de recuperación de la información, ya que se entrega una colección de datos ya procesada, la cual es representada por las secuencias de accesos de usuarios. 
Dicho de otro modo, dada una secuencia de acceso por un usuario que entra a la web, sus accesos web determinarán cual será la predicción de su sesión. Dentro de los registros existen respaldos de sesiones de usuarios, páginas no encontradas, accesos denegados y otra información en relación al funcionamiento. Sin importar el tipo de servidor que utilicemos podemos identificar a usuarios con distintas técnicas y/o combinaciones, por ejemplo, podemos interpretar que el \emph{request} que ha realizado el usuario más su sesión, nos da la información necesaria para poder crear un modelo predictivo. 

Dado lo anterior podemos hacer minería de los datos que un servidor ha recolectado durante un periodo de tiempo, de lo anterior  podemos mencionar que existen varias técnicas para poder hacer ``\emph{Web Access Pattern}'' (\texttt{WAP}) y también ``\emph{Web Usage Minning}'' (\texttt{WUM}), de sus siglas en inglés es minería del uso \emph{web}. No abordaremos este tema pero si lo utilizaremos para poder dar base a nuestro estudio sobre las predicciones en secuencias discretas de acceso, con el objetivo de poder predecir el siguiente acceso que tiene un usuario al visitar un sitio \emph{web}.


%  Applications involving sequential data may require pre- diction of new events, generation of new sequences, or decision making such as classification of sequences or sub-sequences. The classic application of discrete sequence prediction algo- rithms is lossless compression (Bell et al., 1990), but there are numerous other applications involving sequential data, which can be directly solved based on effective prediction of dis- crete sequences. Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Schu ̈tze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classifica- tion (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).1





\section{Predictores de estado finito}


 

Sea $\Sigma$ un alfabeto finito y $q_{i}^{n}$ el $i\mbox{-ésimo}$ símbolo $q$ de un conjunto total de $n$ símbolos. Para entrenar un secuencia $q_{1}^{n}=q_{1},q_{2} \dots q_{n}$, donde $q_{i} \in \Sigma$ y el símbolo compuesto $q_{i}q_{i+1}$, que es la concatenación de $q_{i}$ y $q_{i+1}$, debemos tener un algoritmo que permita leer y usar esta secuencia, además entrenar con los datos procesados y entregar un resultado predictivo con el modelo que se desea generar. El objetivo es entrenar un modelo $M$ que entregue como resultado la probabilidad condicionada para cualquier futuro símbolo dado a una sub-secuencia que ha sido parte del entrenamiento previamente.
Definimos $\Sigma^{*}$ como un conjunto de sub-secuencias de símbolos $\sigma$ que componen $s$, talque $s \ \in \Sigma^{*} $. Específicamente, en cualquier contexto de secuencia $s$ de $\Sigma^{*}$  y  $\sigma \in \Sigma$ símbolos, el aprendizaje de la secuencia dado por el entrenamiento debe dar una distribución de probabilidad $M(\sigma | s )$ que su valor resultado es representada por el modelo $M$ al momento que es consultado o evaluado para predecir.

El rendimiento del modelo predictivo se puede medir mediante una función del promedio de registro de errores $L(M,x_{1}^{T})$ de $M (\cdot | \cdot )$, con respecto a una secuencia $s = x_{1}^{T}$ con $x_{1}^{T}= x_{1},x_{2},....,x_{n} $ %, al acierto del siguiente símbolo $q_{i}$ de la secuencia $s$
  por lo tanto podemos definir $L$ como \begin{equation} L( M , x_{1}^{T} ) = 
- \dfrac{1}{T} 
\sum _{i=1}^{T} \log{ M(x_{i} | x_{i} \cdots x_{i-1}} ),\end{equation}
donde el logaritmo es en base $2$.  El promedio de $L$ es directamente relacionado a  \begin{equation}M(x_{1}^{R}) = \prod_{i=1}^{T} M(x_{i} | x_{i} \cdots x_{i-1} ) \end{equation} y minimizar el promedio de $L$ es completamente equivalente a maximizar la asignación de probabilidades a una secuencia de pruebas $x_{1}^{T}= x_{1},x_{2},\cdots,x_{n} $, teniendo en cuenta que esta equivalencia es totalmente válida. 
Sea $M(x_{1}^{T})$ una asignación de probabilidad consistente para una secuencia completa, la cual satisface \begin{equation}
M(x_{1}^{t-1}) = \sum_{\mbox{$x_t$} \in \Sigma}^{T} M(x_{1} \cdots x_{t-1}x_{t} ) \ , 
\end{equation}para todo $t=1,\cdots\ ,\ T$, induce la asignación de probabilidad,

\begin{equation}
M(x_{t} | x_{1}^{t-1} ) =  \dfrac{M(x_{1}^{t})}{M(x_{1}^{t-1} )},\ t=1,...,T.
\end{equation}


El registro de pérdida $L$ tiene variadas interpretaciones. Tal vez la más importante se encuentra en su equivalencia a \texttt{LDC}. La cantidad $-\log M (x_{i} | x_{1} \cdots x_{i-1}) $, que también se llama la ``auto--información'', puede ser la de compresión ideal o ``largo de secuencia'' de $x_{i}$, en \emph{bits} por símbolo, con respecto a la distribución de probabilidad condicional  \begin{equation}M (X | x_{1} \cdots x_{i-1}) \ ,\end{equation} esta puede ser  \emph{online} (con  una pequeña redundancia arbitraria) usando codificación aritmética (Rissanen y Langdon, 1979)\cite{RissanenLangdon1979}.


Por lo tanto, el promedio de $L$ también mide la tasa de compresión media de una secuencia de prueba, cuando se utilizan las predicciones generadas por $M$, es decir, un bajo promedio de $L$ %log-loss function
sobre la secuencia $x_{1}^{T}$ puede implicar una buena compresión de esta secuencia \cite{Begleiter2004}.

Si se supone que el entrenamiento y las secuencias de pruebas fueron generados de una fuente desconocida\footnote{Mas adelante usaremos el término en ingles \emph{Data Source}, para referirnos a fuentes de datos , tanto conocidas como desconocidas.} $P$. Definimos una secuencia dada por valores aleatorios $X_{1}^{T} = X_{1} \cdots X_{T} $, podemos decir que claramente la distribución $P$ minimiza unicamente \emph{log-loss} o como la hemos llamado anteriormente $L$, lo cual es \begin{equation}
P = arg\ min_{M} \{ - E_{P} \{\log M( X_{1}^{T} )\}   \}
\end{equation}


Dada la equivalencia de \emph{log-loss} y la compresión, como se ha visto anteriormente, el significado de \emph{log-loss} de $P$ logra la mejor compresión posible, o logra una entropía 
\begin{equation}
	H_{T}(P) = - E \log P( X_{1}^{T} ) 
\end{equation}
Aún no conociendo realmente cual es la distribución de probabilidad de $P$, un entrenamiento genera una aproximación a $M$ usando una secuencia de entrenamiento. La pérdida extra que podemos obtener la llamaremos \emph{Redundancia} y esta dada por el valor de

\begin{equation}
D_{T} ( P || M ) = E_{P} \{ - \log M(X_{1}^{T} - (- \log P(X_{1^{T}})   )  )       \} \ .
\end{equation}
 

Para normalizar la \emph{redundancia} $D_{T} ( P || M ) / T $, de una secuencia de largo $T$, da los  \emph{bits} extra por símbolo (sobre la tasa de la entropía) al comprimir una secuencia utilizando $P$.  

Este ajuste probabilístico motiva un objetivo deseable, al entregar un algoritmo de propósito general para la predicción: minimizar la redundancia de manera uniforme, con respecto a todas las posibles distribuciones. 

Un algoritmo de predicción el cual pueda acotar la redundancia de manera uniforme, con respecto a todas las distribuciones dada una clase debe poseer un cota inferior de redundancia para cualquier \emph{Predictor Universal} y \emph{Compresor Universal} \begin{equation}
\Omega \left(  K \dfrac{\log T}{2 T } \right),
\end{equation} donde $K$ es (más o menos) el número de parámetros del modelo que codifica la distribución $P$ (Rissanen \cite{Rissanen1984}, 1984).






 

Si llamamos al siguiente símbolo $b_{t}$, diremos que el resultado de nuestro predictor es entregar este valor. Dado esto, existe una función de pérdida asociada $L( b_{t},x_{t} )$ para cada predicción realizada. 

El objetivo de cada predictor es tener una función de minimización tal que minimize la fracción de predicciones erróneas, a lo anterior lo llamaremos $T$.% que será:

%$$ T = \dfrac{1}{n} \sum^{n}^{t=1} {L( b_{t},x_{t} ) }  $$



% Given the resources in a practical situation, the predictor that is capable of possibly meeting these
% requirements must be a member of the set of all possible finite state machines (FSM’s). Consider the set of all possible finite state predictors with S states. Then the S-state predictability of the sequence xn (denoted by π S (x n ) ), is defined as the minimum fraction of prediction errors made by an FS predictor with S-
% states. This is a measure of the performance of the best possible predictor with S states, with reference to a given sequence. For a fixed-length sequence, as S is increased, the best possible predictor for that sequence will eventually make zero errors. The finite state predictability for a particular sequence is then defined as the S – state predictability for very large S, and very large n, i.e. the finite state predictability of a particular sequence is
% lim limπS(xn). S→∞ n→∞
% FS predictability is an indicator of the best possible sequential prediction that can be made on an arbitrarily long sequence of input symbols by any FSM. This quantity is analogous to FS compressibility, as defined in (Ref. 5), where a value of zero for the FS predictability indicates perfect predictability and a value of 1⁄2 indicates perfect unpredictability.
% This notion of predictability enables a different optimal FS predictor for every individual sequence, but it has been shown in (Ref. 1) that there exist universal FS predictors that, independent of the particular sequence being considered, always attain the FS predictability.






\section{Modelos tradicionales}
%Gueniche_Fournier-Viger_Raman_Tseng
%However, these models suffer from some important limitations [5]. First, most of them assume the Markovian hypothesis that each event solely depends on the previous events. If this hypothesis does not hold, prediction accuracy using these models can severely decrease [5, 3]. Second, all these models are built using only part of the information contained in training sequences. Thus, these models do not use all the information contained in training sequences to perform predic- tions, and this can severely reduce their accuracy. For instance, Markov models typically considers only the last k items of training sequences to perform a pre- diction, where k is the order of the model. One may think that a solution to this problem is to increase the order of Markov models. However, increasing the order of Markov models often induces a very high state complexity, thus making them impractical for many real-life applications [3].

%
%Primero asumimos que cada evento por si 
 

Se ha estudiado como los eventos secuenciales que se obtienen de usuarios \emph{web} cuando navegan se puede modelar, pero reconocer la frecuencia de estos patrones de \emph{webaccess} puede requerir mucha  información de entrada y un alto costo procesamiento.  Indudablemente puede entregar mejor precisión en el resultado de como es el comportamiento de los usuarios, pero no es óptimo para las demandas actuales. 

Estos enfoques tradicionales usan Modelos de Markov, a medida que va aumentando el historial de navegación secuencial el orden de complejidad aumenta directamente proporcional. Lo anterior genera cálculos  complejos y lentos de procesar. Se ha encontrado que la implementación de estos modelos son inadecuados para este propósito~\cite{Dongshan2002} , debido a que no logran abarcar un volumen de datos, ni secuencias de mayores tamaños. Además la escalabilidad de estas propuestas  como señala Dongshan \etal~\cite{Dongshan2002} presentan en la práctica muchas limitaciones técnicas que no permiten ser implementadas. 


 

%CITED Moghaddam_Kabir
%The techniques that rely on sequential patterns such as Markov models and sequential association rules mining contain more precise information about users’ navigation behavior. Association rules were proposed to capture the co-occurrence of buying different items in a supermarket shopping [11]. Association rules indicate groups that are presented together. In [12] study on different kinds of sequential association rules for web document prediction is proposed. It shows how to construct the association rule based prediction models for web log data.




%@TODO
 
 
 \subsection{Limitaciones de los modelos tradicionales de Markov}
 
 %%%%% Cita pagina 2 Dongshan2002 %%%%%%%%
 Los modelos tradicionales de Markov predicen la siguiente página \emph{web} que un usuario puede acceder considerando el acceso más probable, se itera para  coincidir su secuencia de acceso actual con secuencias de accesos \emph{web} históricas.
 
 Usando estos modelos los investigadores como  Dongshan \etal~\cite{Dongshan2002} han comparado  el máximo número de elementos  prefijos de cada secuencia histórica,  con los elementos sufijos de la  misma longitud de secuencia de \emph{webaccess} actual del usuario y obteniendo secuencias dada su secuencia histórica con probabilidad más alta en la que los elementos coinciden.
 
 El modelo de Markov de orden cero, es la tasa base de probabilidad incondicional, dada por \begin{equation}
p(x_n) = Pr(X_n) , \end{equation} que es la probabilidad de visitar un página \emph{web}. El siguiente modelo de Markov de orden uno, observa la probabilidad en la transición de un página a otra, la podemos definir como  $x_{1}$ y $x_{2}$, la primera y segunda página visitada por un usuario. La probabilidad de la transición de una página a otra página sería \begin{equation} 
	 p(x_{2} | x_1) = Pr(X_2 = x_{2} | X_1 = x_{1}) 
 \end{equation}	
 
 El $k$-ésimo orden del Modelo de Markov considera la probabilidad condicional que un usuario cambie a una $n$-ésima página nueva dada su anterior visita, teniendo que $k = n -1$ páginas vistas:

 \begin{equation}\label{eq:tantito}
 \scriptstyle
 p( x_{n} | x_{n-1},..., x_{n-k} ) = Pr(X{n} = x_{n}\ |\ X_{n-1} = x_{n-1},..., X_{n-k} = x_{n-k}) .
 \end{equation}
 
 Los Modelos de Markov de orden inferior no pueden predecir con certeza total el futuro de los \emph{webaccess log}, ya que no van lo suficientemente atrás en el historial de \emph{webaccess} para determinar como se comporta el usuario. Estos comportamiento de los usuarios requieren buenas predicciones, las que a la vez requieren modelos de Markov de mayor orden, pero los modelos de orden superior resultan de mayor complejidad en espacio de estado y cobertura de estados de transición. 
Los modelos con mayor orden de estados son distintas combinaciones de las acciones observadas y registradas en el conjunto de datos de los \emph{webaccess log}, entonces, el número de estados tiende a crecer exponencialmente al igual que el orden del modelo.
 
Este aumento puede limitar significativamente la aplicabilidad de los modelos de Markov para aplicaciones en las que las predicciones rápidas son críticas para el rendimiento en tiempo real o para aplicaciones con restricciones de uso de memoria. Además, muchos ejemplos en los conjuntos de prueba podrían no tener estados correspondientes en los modelos de Markov que da mayor orden, por lo que reduciría su alcance y eficacia.


\input{predictionio} 	 %introducción a las Prediccion