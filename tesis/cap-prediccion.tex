\chapter[Predicciones sobre Web Access]{Predicciones sobre Web Access} \label{ch:tema}



%Las Predicciones en los registros webaccess ha atraido mucha atencion en los últimos años

%In web page prediction understanding the user navigation pattern and then predicting the next pages is the main problem.

%Cita a @Gopalratnam&Cook
Dado que en los nuevos sistemas es cada vez más común conocer sistemas inteligentes y diversos en variadas áreas, sistemas que tienen cualidades como tener la posibilidad de predecir ocurrencia de eventos para así adaptarse y tener versatilidad al tomar decisiones en variadas situaciones. Aún mas necesaria es esta propiedad en problemas que requieren predicciones secuenciales, es decir, dada una secuencia de eventos,  predecir el siguiente evento basado en nuestro conocimiento histórico limitado.
En los últimos años se ha observado que  los contenido de muchos sitios web son dinámicos y nuevas páginas también se añaden al sitio de forma dinámica. Así es como  se hace necesario un modelo predictivo que pueda servir como un modelo predictivo online,  que considere tanto los cambios web que se van produciendo, como también el comportamiento de los usuarios que interactúan con ella. 
Moghaddam \etal~\cite{Moghaddam2009} proponen un modelo predictivo online que cubre la eficiencia de la memoria como un factor importante para un algoritmo en línea. 

Para cualquier secuencia de eventos, estas se pueden modelar como procesos estocásticos, estos algoritmos emplean Modelos de Markov para optimizar las predicciones del siguiente símbolo, en cualquier secuencia estocástica. Otros escenarios requieren que un algoritmo de predicción sea capaz de incrementar su recuperación de información  y poder dar resultados de forma inmediata, es decir predicciones $online$ .

El problema de la predicción secuencial se puede establecer de la siguiente manera, dado un secuencia de símbolos $x_{1}, x_{2}.....x_{n}$, ¿Cuál es el siguiente símbolo $x_{i+1}$? Este problema ha sido formulado gracias al trabajo de  Risseman\cite{Rissanen1983} y Langdom\cite{Langdon1983}.  %Falta cita
un buen compresor es aquel que al comprimir datos sin pérdida se aproxima a un \emph{predictor}.

 




%Also, it has been shown that a predictor with an order that grows at a rate approximating the entropy rate of the source is an optimal predictor. Another motivation to look to the field of text compression is that such algorithms are essentially incremental parsing algorithms, which is an extremely desirable quality in our search for an online predictor. Active LeZi is a predictor addressing this prediction problem, and is based on the above-mentioned motivations.


%CITED Gueniche_Fournier-Viger_Raman_Tseng
%Sequence prediction is an important task with many applications [1, 11]. Let be an alphabet of items (symbols) Z = {e1, e2, ..., em}. A sequence s is an ordered list of items s = ⟨i1,i2,...in⟩, where ik ∈ Z (1 ≤ k ≤ n). A prediction model is trained with a set of training sequences. Once trained, the model is used to perform sequence predictions. A prediction consists in predicting the next items of a sequence. This task has numerous applications such as web page prefetching, consumer product recommendation, weather forecasting and stock market prediction [1, 3, 8, 11].


%% ¿Por que predicciones en web access?




% Un string S = S1,n = S[1, n] = s1, s2, . . . , s es una secuencia de s ımbolos, donde cada s ımbolo pertenece al alfabeto Σ de taman ̃o σ. Un substring de S se escribe Si,j = S[i,j] = sisi+1...sj Un prefijo de S es un substring de la forma S1,j y un sufijo es un substring de la forma Si,n. Si i > j entonces Si,j = ε, el string vac ıo, de largo |ε| = 0. Un texto T = T1,n es un string terminado con un s ımbolo especial tn = $ ̸∈ Σ, lexicogr aficamente menor que cualquier otro s ımbolo en Σ. El orden lexicogr afico (<) entre strings se define como aX < bY si a < b ∨ (a = b ∧ X < Y ), donde a, b son s ımbolos y X, Y son strings sobre Σ.

Para poder crear un modelo predictivo acorde a la Teoría de la Información, un predictor que construye un modelo cuya entropía se aproxima a la de la fuente de datos, consigue una mayor precisión predictiva. 


Las predicciones son un área importante dentro del dominio de las Machine Learning y la Inteligencia Artificial, las cuales pueden ofrecer un sistema de inteligencia que las aplicaciones necesitan para un óptimo desempeño, también ayudan a dar información para la toma de decisiones. Ciertos dominios requieren que la predicción se pueda realizar en las secuencias de eventos que, por lo general, se pueden modelar como un proceso estocástico. 
Nuestro interés se centra en las predicciones de secuencias discretas, y en este punto, demostrar la convergencia en la  cual un modelo de compresión (como el caso de LZ78 que se explicará más detalladamente en el Capítulo 4) y la eficiencia de un algoritmo de compresión, ofrece una nueva perspectiva a las predicciones. 


% @TODO: hacer cita
% ACTIVE LEZI: AN INCREMENTAL PARSING ALGORITHM FOR SEQUENTIAL PREDICTION

Nos enfocaremos en el caso de los acceso que un usuario realiza a un sitio web, el tiempo en el que pasa en este es registrado por el servidor, en esta investigación no se requiere indagar en temas de \emph{Information Retrieval}, ya que se entrega una colección de datos ya procesada, la cual es representada por las secuencias de acceso por parte de usuarios. 
Dicho de otro modo, dada una secuencia de acceso por un usuario que entra a la web, sus accesos web determinarán cual será la predicción de su sesión. Dentro de los registros existen respaldos de sesiones de usuarios, páginas no encontradas, accesos denegados y otra información en relación al funcionamiento. Sin importar el tipo de servidor que utilicemos podemos identificar a usuarios con distintas técnicas y/o combinaciones, por ejemplo, podemos interpretar que el \emph{request} que ha realizado el usuario más su sesión, nos da la información necesaria para poder crear un modelo predictivo. 

Dado lo anterior podemos hacer minería de los datos que un servidor ha recolectado durante un periodo de tiempo, de lo anterior  podemos mencionar que existen varias técnicas para poder hacer \emph{Web Access Pattern } y también  "Web Usage Minning", de sus siglas en inglés es minería del uso de una \emph{web}. Nuestro interés no es abordar este tema pero si explicarlo para poder realizar un estudio predictivo de secuencias de acceso, con el objetivo de poder predecir el siguiente comportamiento que tiene un usuario al momento de visitar un sitio web.


%  Applications involving sequential data may require pre- diction of new events, generation of new sequences, or decision making such as classification of sequences or sub-sequences. The classic application of discrete sequence prediction algo- rithms is lossless compression (Bell et al., 1990), but there are numerous other applications involving sequential data, which can be directly solved based on effective prediction of dis- crete sequences. Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Schu ̈tze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classifica- tion (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).1





\section{Predictores de Estado finito}


 

Sea $\Sigma$ un alfabeto finito. Para poder entrenar un secuencia $q_{1}^{n}=q_{1}q_{2} \dots q_{n}$, donde $q_{i} \in \Sigma$ y $q_{i}q_{i+1}$ es la concatenación de los símbolos $q_{i}$ y $q_{i+1}$. Dado lo anterior el objetivo es poder entrenar un modelo $M$ que entregue como resultado la probabilidad respectiva de cualquier futuro símbolo dado algún pasado. Específicamente, para cualquier contexto de secuencia $s \ \in \Sigma^{*} $ y un símbolo $\sigma \in \Sigma$, el aprendizaje de la secuencia dado por el entrenamiento debe dar una distribución de probabilidad $M(\sigma | s )$.
El rendimiento del modelo predictivo se puede medir mediante una función del promedio de registro de errores $L(M,x_{1}^{T})$ de $M (\cdot | \cdot )$, con respecto a una secuencia $s = x_{1}^{T}$ con $x_{1}^{T}= x_{1},x_{2},....,x_{n} $%, al acierto del siguiente símbolo $q_{i}$ de la secuencia $s$
por lo tanto podemos definir $L$ como

\begin{equation} L( M , x_{1}^{T} ) = 
- \dfrac{1}{T} 
\sum _{i=1}^{T} \log{ M(x_{i} | x_{i} \cdots x_{i-1}} )\end{equation},
donde el logaritmo es en base 2. 

El promedio de $L$ es directamente relacionado a $M(x_{1}^{R}) = \prod_{i=1}^{T} M(x_{i} | x_{i} \cdots x_{i-1} ) $ y minimizar el promedio de $L$ es completamente equivalente a maximizar la asignación de probabilidades a una secuencia de pruebas $x_{1}^{T}= x_{1},x_{2},....,x_{n} $, teniendo en cuenta que esta equivalencia es totalmente válida. Sea $M(x_{1}^{T})$ una asignación de probabilidad consistente para una secuencia completa, la cual satisface que \begin{equation}
M(x_{1}^{t-1}) = \sum_{\mbox{$x_t$} \in \Sigma}^{T} M(x_{1} \cdots x_{t-1}x_{t} )
\end{equation}, para todo $t=1,...,T$, induce la asignación de probabilidad,

\begin{equation}
M(x_{t} | x_{1}^{t-1} ) = M(x_{1}^{t}) / M(x_{1}^{t1}),\ t=1,...,T 
\end{equation}.


El registro de pérdida $L$ tiene variadas interpretaciones. Tal vez la más importante se encuentra en su equivalencia a la compresión sin pérdidas \emph{LDC}. La cantidad $- \log M (x_{i} | x_{1} \cdots x_{i-1}) $, que también se llama la $auto-información$, puede ser la de compresión ideal o "largo de secuencia" de $x_{i}$, en \emph{bits} por símbolo, con respecto a la distribución de probabilidad condicional  \begin{equation}M (X | x_{1} \cdots x_{i-1})\end{equation}, esta puede ser implementada \emph{online} (con  pequeña redundancia arbitraria) usando codificación aritmética (Rissanen y Langdon, 1979)\cite{RissanenLangdon1979} .


Por lo tanto, el promedio de $L$ también mide la tasa de compresión media de una secuencia de prueba, cuando se utilizan las predicciones generadas por $M$, es decir, un bajo promedio de $L$ %log-loss function
sobre la secuencia $x_{1}^{T}$ puede implicar una buena compresión de esta secuencia \cite{Begleiter2004}.

Suponiendo que los entrenamientos y las secuencias de pruebas fueron generados de una fuente desconocida\footnote{Mas adelante usaremos el término en ingles Data Source, para referirnos a fuentes de datos , tanto conocidas como desconocidas.} $P$. Sea una secuencia dada por valores aleatorios $X_{1}^{T} = X_{1} \cdots X_{T} $, podemos decir que claramente la distribución $P$ minimiza unicamente \emph{log-loss} o como la hemos llamado anteriormente $L$, lo cual es

\begin{equation}
P = arg\ min_{M} \{ - E_{P} \{\log M( X_{1}^{T} )\}   \}
\end{equation}


Dada la equivalencia de \emph{log-loss} y la compresión, como se ha visto anteriomente, el significado de \emph{log-loss} de $P$ logra la mejor compresión posible, o logra una entropía 
\begin{equation}
	H_{T}(P) = - E \log P( X_{1}^{T} )
\end{equation}. Aún no conociendo realmente cual es la distribución de probabilidad de $P$, un entrenamiento genera una aproximación a $M$ usando una secuencia de entrenamiento. La pérdida extra que podemos obtener la llamaremos \emph{Redundancia} y esta dada por el valor de

\begin{equation}
D_{T} ( P || M ) = E_{P} \{ - \log M(X_{1}^{T} - (- \log P(X_{1^{T}})   )  )       \} 
\end{equation}.
 

Para normalizar la \emph{redundancia} $D_{T} ( P || M ) / T $, de una secuencia de largo $T$, da los  \emph{bits} extra por símbolo (sobre la tasa de la entropía) al comprimir una secuencia utilizando $P$.  

Este ajuste probabilístico motiva un objetivo deseable, al entregar un algoritmo de propósito general para la predicción: minimizar la redundancia de manera uniforme, con respecto a todas las posibles distribuciones. Un algoritmo de predicción el cual pueda acotar la redundancia de manera uniforme, con respecto a todas las distribuciones dada una clase.

Una cota inferior de la redundancia para cualquier \emph{Predictor Universal} y \emph{Compresor Universal} \begin{equation}
\Omega( K \dfrac{\log T}{2 T } )
\end{equation}, donde $K$ es (más o menos) el número de parámetros del modelo que codifica la distribución $P$ (Rissanen \cite{Rissanen1984}, 1984).






 

Si llamamos al siguiente símbolo $b_{t}$, diremos que el resultado de nuestro predictor es entregar este valor. Dado esto, existe una función de pérdida asociada $L( b_{t},x_{t} )$ para cada predicción realizada. 

El objetivo de cada predictor es tener una función de minimización tal que minimize la fracción de predicciones erróneas, a lo anterior lo llamaremos $T$.% que será:

%$$ T = \dfrac{1}{n} \sum^{n}^{t=1} {L( b_{t},x_{t} ) }  $$



% Given the resources in a practical situation, the predictor that is capable of possibly meeting these
% requirements must be a member of the set of all possible finite state machines (FSM’s). Consider the set of all possible finite state predictors with S states. Then the S-state predictability of the sequence xn (denoted by π S (x n ) ), is defined as the minimum fraction of prediction errors made by an FS predictor with S-
% states. This is a measure of the performance of the best possible predictor with S states, with reference to a given sequence. For a fixed-length sequence, as S is increased, the best possible predictor for that sequence will eventually make zero errors. The finite state predictability for a particular sequence is then defined as the S – state predictability for very large S, and very large n, i.e. the finite state predictability of a particular sequence is
% lim limπS(xn). S→∞ n→∞
% FS predictability is an indicator of the best possible sequential prediction that can be made on an arbitrarily long sequence of input symbols by any FSM. This quantity is analogous to FS compressibility, as defined in (Ref. 5), where a value of zero for the FS predictability indicates perfect predictability and a value of 1⁄2 indicates perfect unpredictability.
% This notion of predictability enables a different optimal FS predictor for every individual sequence, but it has been shown in (Ref. 1) that there exist universal FS predictors that, independent of the particular sequence being considered, always attain the FS predictability.






\section{Modelos tradicionales}
%Gueniche_Fournier-Viger_Raman_Tseng
%However, these models suffer from some important limitations [5]. First, most of them assume the Markovian hypothesis that each event solely depends on the previous events. If this hypothesis does not hold, prediction accuracy using these models can severely decrease [5, 3]. Second, all these models are built using only part of the information contained in training sequences. Thus, these models do not use all the information contained in training sequences to perform predic- tions, and this can severely reduce their accuracy. For instance, Markov models typically considers only the last k items of training sequences to perform a pre- diction, where k is the order of the model. One may think that a solution to this problem is to increase the order of Markov models. However, increasing the order of Markov models often induces a very high state complexity, thus making them impractical for many real-life applications [3].

%
%Primero asumimos que cada evento por si 
En secciones anteriores se ha hablado de como los eventos secuenciales se pueden modelar como un proceso estocástico.

Ciertamente para reconocer frecuencia en patrones secuenciales y reglas de asociación puede requerir mucha  información y entregar mejor precisión en la información de los usuarios y su comportamiento en la web. Pero un enfoque en que la temporalidad de los datos se debe modelar con un modelo de Markov es la clave para poder realizar esto, y estos datos ya han ayudado a predecir los accesos de los usuarios, como señala Dongshan \etal~\cite{Dongshan2002} pero en la práctica existen muchas limitaciones técnicas que no permiten que se implementen. 


 

%CITED Moghaddam_Kabir
%The techniques that rely on sequential patterns such as Markov models and sequential association rules mining contain more precise information about users’ navigation behavior. Association rules were proposed to capture the co-occurrence of buying different items in a supermarket shopping [11]. Association rules indicate groups that are presented together. In [12] study on different kinds of sequential association rules for web document prediction is proposed. It shows how to construct the association rule based prediction models for web log data.




%@TODO
 
 
 \subsection{Limitaciones de los modelos tradicionales de Markov}
 
 %%%%% Cita pagina 2 Dongshan2002 %%%%%%%%
 
 Los modelos tradicionales de Markov predicen la siguiente página Web que un usuario puede acceder considerando el acceso más probable, se itera para  coincidir su secuencia de acceso actual con secuencias de accesos Web históricas.
 
 
 Usando estos modelos los investigadores como  Dongshan \etal~\cite{Dongshan2002} han comparado  el máximo de elementos  prefijos de cada secuencia histórica de webaccess,  con los elementos sufijos de la  misma longitud de secuencia de webaccess actual del usuario y obteniendo secuencias dado su secuencia histórica con la probabilidad más alta en la cual los elementos coinciden.
 
 El modelo de Markov de orden cero es la tasa base de probabilidad incondicional, la cual es la probabilidad de la página visitada, dada por
 \begin{equation}
p(x_n) = Pr(X_n),
 \end{equation}	
 donde $x_{n}$ es xn y $Xn$ es otra Xn.
 
 El modelo de Markov de orden uno observa la probabilidad de la transición de un página a otra, la podemos interpretar así:
 
 \begin{equation} 
	 p(x_{2} | x_1) = Pr(X_2 = x_2 | X_1 = x_1) 
 \end{equation}	
 
 El K-ésimo orden del Modelo de Markov considera la probabilidad condicional que un usuario cambie a una nueva  n-ésima página  dado su anterior página visitada, teniendo que $k = n -1$ páginas vistas:
 
 \begin{equation}\label{eq:tantito}
p( x_{n} | x_{n-1},..., x_{n-k} ) = Pr(X{n} = x_{n} | X_{n-1} = x_{n-1},..., X_{n-k} = x_{n-k}) 
 \end{equation}


 
 
 Modelos de Markov, en (\ref{eq:tantito}), de orden inferior no pueden predecir con éxito total el futuro de los webaccess log, ya que no van lo suficientemente atrás en el pasado para discriminar el modo en que se comporta el usuario. Este comportamiento de los usuarios requiere de buenas predicciones, las que a la vez requieren modelos de Markov de orden superior, pero los modelos de orden superior resultan de alta complejidad en espacio de estado y cobertura. 
 
Modelos con mayor orden de estados son distintas combinaciones de las acciones observadas en un set de datos, entonces, el número de estados tiende a crecer exponencialmente al igual que el orden del modelo. 
Este aumento puede limitar significativamente la aplicabilidad de los modelos de Markov para aplicaciones en las que las predicciones rápidas son críticas para el rendimiento en tiempo real o para aplicaciones con restricciones de uso de memoria. Además, muchos ejemplos en los test podrían no tener estados correspondientes en los modelos de Markov de mayor orden, por lo que reduciría su alcance.

