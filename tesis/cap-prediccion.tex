\chapter[Predicciones sobre Web Access]{Predicciones sobre Web Access} \label{ch:tema}



%Las Predicciones en los registros webaccess ha atraido mucha atencion en los últimos años

%In web page prediction understanding the user navigation pattern and then predicting the next pages is the main problem.

%Cita a @Gopalratnam&Cook
Dado los nuevos sistemas es cada vez más común conocer sistemas Inteligentes y diversos en variadas áreas, una de estas cualidades de los nuevos sistemas este tener las posibilidad de predecir ocurrencia de eventos para poder adaptarse y tener versatilidad al tomar decisiones en variadas situaciones. Aún mas necesaria es esta propieda en problemas que requieren predicciones secuenciales, es decir dada una secuencia de eventos, como poder predecir el siguiente evento basado en nuestro conocimiento histórico limitado.

En los últimos años el contenido de muchos sitios web son dinámicos y nuevas páginas también se añaden al sitio de forma dinámica. Por lo cual un modelo predictivo que pueda servir como un modelo predictivo \emph{online} e ir considerando los cambios web que se van produciendo, como también el comportamiento de los usuarios que interactúan con ella.

En \cite{Moghaddam2009} proponen un modelo predictivo online que cubre la eficiencia de la memoria como un factor importante para un algoritmo en línea. 



Para cualquier secuencia de eventos, estas se pueden modelar como proceso estocásticos, esto algoritmos emplean Modelos de Markov para optimizar las predicciones del siguiente símbolo, en cualquier secuencia estocástica. Otros escenarios requieren que algoritmo de predicción sea capaz de incrementar su recuperación de información  y poder dar resultados de forma inmediata, es decir predicciones $online$ .

El problema de la predicción secuencial se puede establecer de la siguiente manera, dado un secuencia de simbolos $x_{1}, x_{2}.....x_{n}$, ¿Cúal es el siguiente simbolo $x_{i+1}$? Señalado el problema la literatura converge a que gracias Risseman y Langodom
 %Falta cita
un buen compresor, o mejor dicho que cualquier compresor de datos sin pérdida se aproxima a un $predictor$.

Para poder crear un modelo predictivo acorde a la Teoría de la Información, un predictor que construye un modelo cuya entropía se aproxima a la de la fuente de datos, consigue una mayor precisión predictiva.





%Also, it has been shown that a predictor with an order that grows at a rate approximating the entropy rate of the source is an optimal predictor. Another motivation to look to the field of text compression is that such algorithms are essentially incremental parsing algorithms, which is an extremely desirable quality in our search for an online predictor. Active LeZi is a predictor addressing this prediction problem, and is based on the above-mentioned motivations.


%CITED Gueniche_Fournier-Viger_Raman_Tseng
%Sequence prediction is an important task with many applications [1, 11]. Let be an alphabet of items (symbols) Z = {e1, e2, ..., em}. A sequence s is an ordered list of items s = ⟨i1,i2,...in⟩, where ik ∈ Z (1 ≤ k ≤ n). A prediction model is trained with a set of training sequences. Once trained, the model is used to perform sequence predictions. A prediction consists in predicting the next items of a sequence. This task has numerous applications such as web page prefetching, consumer product recommendation, weather forecasting and stock market prediction [1, 3, 8, 11].


%% ¿Por que predicciones en web access?




% Un string S = S1,n = S[1, n] = s1, s2, . . . , s es una secuencia de s ımbolos, donde cada s ımbolo pertenece al alfabeto Σ de taman ̃o σ. Un substring de S se escribe Si,j = S[i,j] = sisi+1...sj Un prefijo de S es un substring de la forma S1,j y un sufijo es un substring de la forma Si,n. Si i > j entonces Si,j = ε, el string vac ıo, de largo |ε| = 0. Un texto T = T1,n es un string terminado con un s ımbolo especial tn = $ ̸∈ Σ, lexicogr aficamente menor que cualquier otro s ımbolo en Σ. El orden lexicogr afico (<) entre strings se define como aX < bY si a < b ∨ (a = b ∧ X < Y ), donde a, b son s ımbolos y X, Y son strings sobre Σ.




Las predicciones son un área importante dentro del dominio de las Machine Learning y la Inteligencia Artificial, las cuales pueden ofrecer un sistema de inteligencia que las aplicaciones necesitan para un optimio desempeño, también ayudan a dar información para la elección de decisiones. Ciertos dominios requieren que la predicción se pueda realizar en las secuencias de eventos que por lo general se puede modelar como un proceso estocástico. 

Nuestro interés se focaliza en las predicciones de sequencias discretas, en el cual queremos demostrar la convergencia en el cual un modelo de compresión, como el caso de LZ78 que se explicará mas detalladamente en el capitulo posterior. La eficiencia de un algoritmo de compresión ofrece una nueva perspectiva a las predicciones, la cual ha despertado un interés en investigadores.
% @TODO: hacer cita
% ACTIVE LEZI: AN INCREMENTAL PARSING ALGORITHM FOR SEQUENTIAL PREDICTION

Nos enfocaremos en el caso del los accesso que un usuario realiza a un sitio web, el tiempo en el que pasa en este es registrado por el servidor, en esta investigación no se requiere indagar en temas de Information Retrieval, ya que se entrega una colección de datos ya procesada, la cual es representa secuencias de acceso por parte de usuarios.




Nuestro interés se presetna en dada un sequencia accesso, un usuario entra accede a la web, dado sus accesos web cual será la predicción de su sesión.  Dentro de los registros existen respaldo de sesiones de usuarios, páginas no encontradas, accesos denegados y otra información en relación al funcionamiento. Sin importar el tipo de servidor que utilicemos podemos identificar a usuarios con distintas técnicas y/o combinaciones, por ejemplo podemos interpretar que el \emph{request} que ha realizado el usuario mas su \emph{session} nos da la información necesaria para poder crear un modelo predictivo.

Dado lo anterior podemos hacer minería de los datos que un servidor ha recolectado durante un periodo de tiempo, de lo anterior  podemos mencionar que existen varias técnicas para poder hacer \emph{Web Access Pattern } y también  "Web Usage Minning", de sus siglas en ingles es minería del uso de una \emph{web}. Nuestro interés no es abordar este tema pero si explicarlo para poder realizar un estudio predictivo de secuencias de acceso, con el objetivo de poder predecir el siguiente comportamiento que tiene un usuario al momento de visitar un sitio web.


%  Applications involving sequential data may require pre- diction of new events, generation of new sequences, or decision making such as classification of sequences or sub-sequences. The classic application of discrete sequence prediction algo- rithms is lossless compression (Bell et al., 1990), but there are numerous other applications involving sequential data, which can be directly solved based on effective prediction of dis- crete sequences. Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Schu ̈tze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classifica- tion (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).1





\section{Predictores de Estado finito}


% %%%%%%% Esto deberia en la seccion de Machine Learning Compress %%%%%%%%%%%%%
% Let Σ be a finite alphabet. A learner is given a training sequence q1n = q1q2 · · · qn, where qi ∈ Σ and qiqi+1 is the concatenation of qi and qi+1. Based on q1n, the goal is to learn a model Pˆ that provides a probability assignment for any future outcome given some past. Specifically, for any “context” s ∈ Σ∗ and symbol σ ∈ Σ the learner should generate a conditional probability distribution Pˆ(σ|s).
% Prediction performance is measured via the average log-loss l(Pˆ,xT1) of Pˆ(·|·), with respect to a test sequence xT1 = x1 ···xT,
% 1 􏰍T
% l(Pˆ,xT1 ) = −T logPˆ(xi|x1 ···xi−1), (1)
% i=1
% where the logarithm is taken to base 2. Clearly, the average log-loss is directly related to the likelihood Pˆ(xT1 ) = 􏰏Ti=1 Pˆ(xi|x1 · · · xi−1) and minimizing the average log-loss is completely equivalent to maximizing a probability assignment for the entire test sequence.
% ￼Note that the converse is also true. A consistent probability assignment Pˆ(xT1 ), for the entire sequence, which satisfies Pˆ(xt−1) = 􏰌 Pˆ(x1 ···xt−1xt), for all t = 1,...,T,
% 1 xt∈Σ induces conditional probability assignments,
% Pˆ(xt|xt−1) = Pˆ(xt1)/Pˆ(xt−1), t = 1, . . . , T. 11
% Therefore, in the rest of the paper we interchangeably consider Pˆ as a conditional distribu- tion or as a complete (consistent) distribution of the test sequence xT1 .
% The log-loss has various meanings. Perhaps the most important one is found in its equivalence to lossless compression. The quantity − log Pˆ(xi|x1 · · · xi−1), which is also called the ‘self-information’, is the ideal compression or “code length” of xi, in bits per symbol, with respect to the conditional distribution Pˆ(X|x1 · · · xi−1), and can be implemented online (with arbitrarily small redundancy) using arithmetic encoding (Rissanen & Langdon, 1979).



Si se tiene un secuencia estocástica, $x_{n} = x , x ,.....x .  $, en un tiempo $t$ el predictor debe saber cual es el siguiente simbolo, basdo en su historia o la frecuencia de ocurrencia que se vaya entrenando.

Si llamamos al siguiente simbolo $b_{t}$, diremos que el resultado de nuestro predictor es entregar este valor. Dado esto, existe una función de perdida asociada $L( b_{t},x_{t} )$ para cada predicción realizada. 

El objetivo de cada predictor es tener una función de minimización tal que minimize la fracción de predicciones erroneas, a lo anterior lo llamaremos $T$ que será:

%$$ T = \dfrac{1}{n} \sum^{n}^{t=1} {L( b_{t},x_{t} ) }  $$



% Given the resources in a practical situation, the predictor that is capable of possibly meeting these
% requirements must be a member of the set of all possible finite state machines (FSM’s). Consider the set of all possible finite state predictors with S states. Then the S-state predictability of the sequence xn (denoted by π S (x n ) ), is defined as the minimum fraction of prediction errors made by an FS predictor with S-
% states. This is a measure of the performance of the best possible predictor with S states, with reference to a given sequence. For a fixed-length sequence, as S is increased, the best possible predictor for that sequence will eventually make zero errors. The finite state predictability for a particular sequence is then defined as the S – state predictability for very large S, and very large n, i.e. the finite state predictability of a particular sequence is
% lim limπS(xn). S→∞ n→∞
% FS predictability is an indicator of the best possible sequential prediction that can be made on an arbitrarily long sequence of input symbols by any FSM. This quantity is analogous to FS compressibility, as defined in (Ref. 5), where a value of zero for the FS predictability indicates perfect predictability and a value of 1⁄2 indicates perfect unpredictability.
% This notion of predictability enables a different optimal FS predictor for every individual sequence, but it has been shown in (Ref. 1) that there exist universal FS predictors that, independent of the particular sequence being considered, always attain the FS predictability.






\section{Modelos tradicionales}
%Gueniche_Fournier-Viger_Raman_Tseng
%However, these models suffer from some important limitations [5]. First, most of them assume the Markovian hypothesis that each event solely depends on the previous events. If this hypothesis does not hold, prediction accuracy using these models can severely decrease [5, 3]. Second, all these models are built using only part of the information contained in training sequences. Thus, these models do not use all the information contained in training sequences to perform predic- tions, and this can severely reduce their accuracy. For instance, Markov models typically considers only the last k items of training sequences to perform a pre- diction, where k is the order of the model. One may think that a solution to this problem is to increase the order of Markov models. However, increasing the order of Markov models often induces a very high state complexity, thus making them impractical for many real-life applications [3].

%
Primero asumimos que cada evento por si 



En secciones anteriores se ha hablado de como los eventos secuenciales se pueden modelar como un proceso estocástico.

Ciertamente el uso de reconocer frecuencia patrones secuenciales, reglas de asociación puede requerir mucha  información y entregar mejor precisión en la información de los usuarios y su comportamiento en la web. Pero un enfoque en que la temporalidad de los datos se debe modelar con un modelo de Markov es la clave para poder realizar esto, y estos ya han ayudado a predecir los accesos de los usuarios, como señala %Referencia a DONGSHAN
pero en la práctica existen muchas limitaciones técnicas que permiten que se implementen. 

%explicar los jodidos de Markov y su matemática
 

%CITED Moghaddam_Kabir
%The techniques that rely on sequential patterns such as Markov models and sequential association rules mining contain more precise information about users’ navigation behavior. Association rules were proposed to capture the co-occurrence of buying different items in a supermarket shopping [11]. Association rules indicate groups that are presented together. In [12] study on different kinds of sequential association rules for web document prediction is proposed. It shows how to construct the association rule based prediction models for web log data.




%@TODO
 
 
 \subsection{Limitaciones de los modelos tradicionales de Markov}
 
 %%%%% Cita pagina 2 Dongshan2002 %%%%%%%%
 
 Los modelos tradicionales de Markov predicen la siguiente página Web que un usuario puede acceder considerando el acceso más probable, iterando coincidir su secuencia de acceso actual con secuencias de acceso Web histórica.
 
 
 Usando estos modelos se ha comparado los investigadores comparan el máximo de elementos  prefijos de cada secuencia histórica de web access con los elementos sufijos de la  misma longitud de secuencia de web access actual del usuario y obteniendo secuencias históricas con la probabilidad más alta de elementos que coinciden.
 
 El modelo de Markov de orden cero es la tasa base de probabilidad incondicional, la cual es la probabilidad de la página visitada, dada por
 \begin{equation}
p(x_n) = Pr(X_n),
 \end{equation}	
 donde $x_{n}$ es xn y $Xn$ es otra Xn.
 
 El modelo de Markov de orden uno observa la probabilidad de la transición de un pagina a otra, la podemos interpretar así:
 
 \begin{equation} 
	 p(x2 | x1) = Pr(X2 = x2 | X1 = x1) 
 \end{equation}	
 
 El K-ésimo orden del Modelo de Markov considera la probabilidad condicional que un usuario cambie a una nueva  n-ésima página  dado su anterior pagina visitada, teniendo que $k = n -1$ paginas vistas:
 
 \begin{equation}\label{eq:tantito}
p( x_{n} | x_{n-1},..., x_{n-k} ) = Pr(X{n} = x_{n} | X_{n-1} = x_{n-1},..., X_{n-k} = x_{n-k}) 
 \end{equation}


 
 
 
 Modelos de Markov, en (\ref{eq:tantito}), de orden inferior no pueden predecir con éxito total el futuro de los web access log, ya que no se ven lo suficientemente atrás en el pasado para discriminar el modo en que se comporta el usuario. Este comportamiento de los usuarios tanto requiere de buenas predicciones los cuales a su vez requieren modelos de Markov de orden superior, pero los modelos de orden superior resultaran de alta complejidad en espacio de estado y cobertura.
 
Modelos con mayor Orden de estados son distintas combinaciones de las acciones observadas en un set de datos, entonces el numero de estados tiende a crecer exponencial-mente al igual que el Orden del modelo.

Este aumento puede limitar significativamente la aplicabilidad de los modelos de Markov para aplicaciones en las que las predicciones rápidas son críticas para el rendimiento en tiempo real o para aplicaciones con restricciones de uso de memoria. Además, muchos ejemplos en los test podrían no tener estados correspondientes en los modelos de Markov de mayor orden, por lo que reduciría su alcance.
 
  %%%%% fin Cita parafraseo pagina 2 Dongshan2002 %%%%%%%%
 

% Genero toda las referencias para demostrar el uso de la bibliografía
% No es necesario que utilice este comando en su documento.
\nocite{*}
