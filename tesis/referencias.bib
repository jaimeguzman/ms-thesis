% Encoding: UTF-8

% %%%%%%%%%%%%
@Book{guller2015big,
  Title                    = {Big Data Analytics with Spark},
  Author                   = {Guller, Mohammed},
  Publisher                = {Springer},
  Year                     = {2015}
}

@Book{gollapudi2016practical,
  Title                    = {Practical Machine Learning},
  Author                   = {Gollapudi, S.},
  Publisher                = {Packt Publishing},
  Year                     = {2016},
  Series                   = {Community experience distilled},
  ISBN                     = {9781784399689},
  URL                      = {https://books.google.cl/books?id=3ywhjwEACAAJ}
}

@Book{khanna2015efficient,
  Title                    = {Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers},
  Author                   = {Khanna, Rahul and Awad, Mariette},
  Publisher                = {Apress},
  Year                     = {2015}
}

@Book{MengyiPu2006,
  Title                    = {Fundamental Data Compression},
  Annote                   = {SIGNATUR = 788.362},
  Author                   = {Ida Mengyi Pu},
  Publisher                = {Elsevier},
  Year                     = {2006},
  Place                    = {Favoritenstrasse 9/4th Floor/1863}
}

% %%%%%%%%%%%%%%

@Book{Navarro2014,
  Title                    = {Teoría de la Computación Apuntes y Ejercicios, Lenguajes Formales,Computabilidad y Complejidad. },
  Author                   = {Gonzalo Navarro},
  Year                     = {2014},
  Place                    = {Departamento de Ciencias de la Computación Universidad de Chile}
}




% %%%%%%%%%%%%%%% %%%%%%%%%%%%%%% %%%%%%%%%%%%%%% %%%%%%%%%%%%%%
@Article{Begleiter2004,
  Title                    = {On Prediction Using Variable Order Markov Models},
  Author                   = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
  Journal                  = {J. Artif. Int. Res.},
  Year                     = {2004},
  Month                    = dec,
  Number                   = {1},
  Pages                    = {385--421},
  Volume                   = {22},
  Abstract                 = {This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a "decomposed" CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.},
  Acmid                    = {1622499},
  Address                  = {USA},
  File                     = {:../docs/paper-tesis/1107.0051.pdf:PDF},
  ISSN                     = {1076-9757},
  Issue_date               = {July 2004},
  Keywords                 = {rank5},
  Numpages                 = {37},
  Publisher                = {AI Access Foundation},
  Review                   = {
  - algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models.

  - Learning of sequential data continues to be a fundamental task and a challenge in pattern recognition and machine learning.

  - La oferta en la literatura es:
  Perhaps the most commonly used techniques are based on Hidden Markov Models (HMMs)

  - In this paper we focus on general-purpose prediction algorithms, based on learning Vari- able order Markov Models (VMMs) over a finite alphabet Σ.

  - There is an intimate relation between prediction of discrete sequences and lossless com- pression algorithms, where, in principle, any lossless compression algorithm can be used for prediction and vice versa (see, e.g., Feder & Merhav, 1994).

  - ¿ Porque se eligio lz78 y no PPM?

  - the goal is to generate a model capable of predicting the rest of the sequence.

  - The ‘probabilistic suffix tree (PST)’ algorithm is well known in the machine learning community.

  - lz78 algorithm that forms the basis of many commercial applications for compression

  -
  3.1 Lempel-Ziv 78 (LZ78)
  The lz78 algorithm is among the most popular lossless compression algorithms (Ziv & Lem- pel, 1978). It is used as the basis of the Unix compress utility and other popular archiving utilities for PCs. It also has performance guarantees within several analysis models. This algorithm (together with the lz77 compresion method) attracted enormous attention and inspired the area of lossless compression and sequence prediction.
  The prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983). The presentation of this algorithm is simplified after the well-known lz78 compression algorithm, which works as follows, is understood. Given a sequence q1n ∈ Σn, lz78 incrementally parses q1n into non-overlapping adjacent ‘phrases’, which are collected into a phrase ‘dictionary’. The algorithm starts with a dictionary containing the empty phrase ǫ. At each step the algorithm parses a new phrase, which is the shortest phrase that is not yet in the dictionary. Clearly, the newly parsed phrase s′ extends an existing
   
  dictionary phrase by one symbol; that is, s′ = sσ, where s is already in the dictionary
  (s can be the empty phrase). For compression, the algorithm encodes the index of s′
  (among all parsed phrases) followed by a fixed code for σ. Note that coding issues will not
  concern us in this paper. Also observe that lz78 compresses sequences without explicit
  probabilistic estimates. Here is an example of this lz78 parsing: if q11 = abracadabra, 1
  then the parsed phrases are a|b|r|ac|ad|ab|ra. Observe that the empty sequence ǫ is always in the dictionary and is omitted in our discussions.
  An lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983). We describe separately the learning and prediction phases.6 For simplicity we first discuss the binary case where Σ = {0, 1}, but the algorithm can be naturally extended to alphabets of any size (and in the experiments discussed below we do use the multi-alphabet algorithm). In the learning phase the algorithm constructs from the training sequence q1n a binary tree (trie) that records the parsed phrases (as discussed above). In the tree we also maintain counters that hold statistics of q1n. The initial tree contains a root and two (left and right) leaves. The left child of a node corresponds to a parsing of ‘0’ and the right child corresponds to a parsing of ‘1’. Each node maintains a counter. The counter in a leaf is always set to 1. The counter in an internal node is always maintained so that it equals the sum of its left and right child counters. Given a newly parsed phrase s′, we start at the root and traverse the tree according to s′ (clearly the tree contains a corresponding path, which ends at a leaf). When reaching a leaf, the tree is expanded by making this leaf an internal node and adding two leaf-sons to this new internal node. The counters along the path to the root are updated accordingly.
  To compute the estimate Pˆ(σ|s) we start from the root and traverse the tree according to s. If we reach a leaf before “consuming” s we continue this traversal from the root, etc. Upon completion of this traversal (at some internal node, or a leaf) the prediction for σ = ‘0’ is the ‘0’ (left) counter divided by the sum of ‘0’ and ‘1’ counters at that node, etc.
  For larger alphabets, the algorithm is naturally extended such that the phrases are
  stored in a multi-way tree and each internal node has exactly k = |Σ| children. In addition,
  each node has k counters, one for each possible symbol. In Figure 1 we depict the resulting
  tree for the training sequence q11 = abracadabra and calculate the probability Pˆ(b|ab), 1
  assuming Σ = {a, b, c, d, r}.
  Several performance guarantees were proven for the lz78 compression (and prediction)
  algorithm. Within a probabilistic setting (see Section 2), when the unknown source is stationary and ergodic Markov of finite order, the redundancy is bounded above by (1/ ln n) where n is the length of the training sequence (Savari, 1997). Thus, the lz78 algorithm is a universal prediction algorithm with respect to the large class of stationary and ergodic Markov sources of finite order.



  -> Modificaciones a LZ78 

  A major advantage of the lz78 algorithm is its speed. This speed is made possible by compromising on the systematic counts of all sub-sequences. For example, in Figure 1, the sub-sequence br is not parsed and, therefore, is not part of the tree. However, this sub- sequence is significant for calculating Pˆ(a|br). While for a very long training sequence this compromise will not affect the prediction quality significantly, for short training sequences the lz78 algorithm yields sparse and noisy statistics. Another disadvantage of the lz78 algorithm is its loss of context when calculating Pˆ(σ|s). For example, in Figure 1, when taking σ = b and s = raa, the lz78 prediction algorithm will compute Pˆ(b|raa) by travers- ing the tree along the raa path (starting from the root) and will end on a leaf. Therefore, Pˆ(b|raa) = Pˆ(b|ǫ) = 5/33. Nevertheless, we could use the suffix a of raa to get the more accurate value Pˆ(b|a) = 5/17. These are two major deficiencies of lz78. The lz-ms al- gorithm attempts to overcome both these disadvantages by introducing two corresponding heuristics. This algorithm improves the lz78 predictions by extracting more phrases during learning and by ensuring a minimal context for the next phrase, whenever possible.



  -> 
  We focused on prediction settings that are more closely related to those required by machine learning practitioners dealing with discrete sequences
  },
  URL                      = {http://dl.acm.org/citation.cfm?id=1622487.1622499}
}

@Article{Chen2011,
  Title                    = {Improve on frequent access path algorithm in web page personalized recommendation model},
  Author                   = {Yuhua Chen and Xin Chen and Haoyi Chen},
  Year                     = {2011},
  Month                    = {March},
  Pages                    = {83-86},
  Abstract                 = {Web logs record actions and behaviors of users. By mining and analyzing these logs we can find users browsing and access patterns, and this is very important and useful to the web site optimization and recommender. This paper first analyses the association-rules-based personalized recommender model which is very popular in web site recommender system, points out the limitation of the frequent access path algorithm in this model, and then improves it. At last, the paper shows by the test results that the improved algorithm can advance the recommending quality.},
  Booktitle                = {Information Science and Technology (ICIST), 2011 International Conference on},
  DOI                      = {10.1109/ICIST.2011.5765216},
  File                     = {:../papers/recomender-system/Improve on Frequent Access Path Algorithm.pdf:PDF},
  Keywords                 = {Web sites, data mining, information retrieval, recommender systems, Web logs, Web page personalized recommendation model, Web site optimization, Web site recommender system, association rules based personalized recommender model, frequent access path algorithm, logs mining, Navigation},
  Owner                    = {jaimeguzman},
  Timestamp                = {2015.07.27}
}

@Article{Claude2014,
  Title                    = {Efficient Indexing and Representation of Web Access Logs},
  Author                   = {Claude, F. and Konow, R. and Navarro, G.},
  Year                     = {2014},
  Abstract                 = {Or to analyze the performance of their systems in order to apply smart prefetching techniques for faster response, among other applications.
  One particular WUM task is to predict the path of web pages that the user is going to traverse within a website.
  Another application is as a recommendation technique

  improving the user’s experience.

  WUM and general data mining is to retrieve the top-k most frequent sequences

  In this paper we focus on improving the space consumption and time to perform queries on the web access sequences obtained from the web server logs used during the first step, thus freeing resources for the second stage and therefore increasing the performance of the process.
  Indexing Web Access Sequences
  ** Our experiments show that our index uses about the same size of the plain representation of the data, and within this space supports various relevant operations within tens of microseconds.
  },
  File                     = {:../papers/fclaude-2014/bwt.pdf:PDF},
  Keywords                 = {Web Usage Minning, WUM Process, server access logs, rank2},
  Owner                    = {jguzman},
  Timestamp                = {2015.07.27}
}

@Article{Domenech2006,
  Title                    = {Web prefetching performance metrics: A survey},
  Author                   = {Josep Domènech and José A. Gil and Julio Sahuquillo and Ana Pont},
  Journal                  = {Performance Evaluation},
  Year                     = {2006},
  Number                   = {9–10},
  Pages                    = {988--1004},
  Volume                   = {63},
  Abstract                 = {Web prefetching techniques have been pointed out to be especially important to reduce perceived web latencies and, consequently, an important amount of work can be found in the open literature. But, in general, it is not possible to do a fair comparison among the proposed prefetching techniques due to three main reasons: (i) the underlying baseline system where prefetching is applied differs widely among the studies; (ii) the workload used in the presented experiments is not the same; (iii) different performance key metrics are used to evaluate their benefits. This paper focuses on the third reason. Our main concern is to identify which are the meaningful indexes when studying the performance of different prefetching techniques. For this purpose, we propose a taxonomy based on three categories, which permits us to identify analogies and differences among the indexes commonly used. In order to check, in a more formal way, the relation between them, we run experiments and estimate statistically the correlation among a representative subset of those metrics. The statistical results help us to suggest which indexes should be selected when performing evaluation studies depending on the different elements in the considered web architecture. The choice of the appropriate key metric is of paramount importance for a correct and representative study. As our experimental results show, depending on the metric used to check the system performance, results cannot only widely vary but also reach opposite conclusions. },
  DOI                      = {http://dx.doi.org/10.1016/j.peva.2005.11.001},
  ISSN                     = {0166-5316},
  Keywords                 = {Web prefetching},
  URL                      = {http://www.sciencedirect.com/science/article/pii/S0166531605001549}
}

@Article{Dongshan2002,
  Title                    = {A new Markov model for Web access prediction},
  Author                   = {Xing Dongshan and Shen Junyi},
  Journal                  = {Computing in Science Engineering},
  Year                     = {2002},
  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {34-39},
  Volume                   = {4},
  Abstract                 = {Accurately predicting Web user access behavior can minimize user-perceived latency, which is crucial in the rapidly growing World Wide Web. Although traditional Markov models have helped predict user access behavior, they have serious limitations. Hybrid-order treelike Markov models predict Web access precisely while providing high coverage and scalability.},
  DOI                      = {10.1109/MCISE.2002.1046594},
  File                     = {:../papers/01046594.pdf:PDF},
  ISSN                     = {1521-9615},
  Keywords                 = {Internet, Markov processes, computer network management, Web user access behavior, World Wide Web, hybrid-order treelike Markov models, scalability, traditional Markov models, user-perceived latency, Delay, Navigation, Predictive models, Prefetching, Scalability, Testing, Traffic control, Web pages, Web sites, rank5},
  Owner                    = {jaimeguzman},
  Review                   = {Web access prediction is impor- tant for networking, data min- ing, e-business, and other areas. HTMM is one of several core components of our developing framework for Web usage mining. We plan to perform exten- sible testing and investigate many possible im- provements. Future work will add more factors to our HTMM, such as navigation time and page content, to more accurately reflect user in- terest. These factors might give us more accu- rate prediction performance for Web access. Another important research direction for this prediction technology is improvement of the practical application, such as prefetching multi- media content.},
  Timestamp                = {2015.07.27}
}

@Article{Eremic2010,
  Title                    = {Mining user access logs to optimize navigational structure of adaptive web sites},
  Author                   = {Eremic, Z. and Radosav, D. and Markoski, B.},
  Year                     = {2010},
  Month                    = {Nov},
  Pages                    = {271-276},
  Abstract                 = {Web sites may contain numerous documents. Using some web techniques, it's possible to analyze users' data about using resources, contents of those documents and structure of web sites. Adaptive web sites automatically change their structure and representation based on visitor's behavior. Shortcutting is an approach that enables connecting two documents which has never been connected before. Most of existing approaches enables connecting the first and the last document in user's navigation path, not considering the possibility that some of the documents within the navigation path might contain useful information for reaching intended document. These documents, which are positioned within user's navigation path, are called wayposts, and they may contain useful information that can help users to get to the specific ìtargetî document. The goal of this paperwork is to discuss about all the possibilities of identifying those waypost documents in users' navigation paths and to propose an optimization of navigation structure of a web site based on users' navigation paths, initial and target documents.},
  Booktitle                = {Computational Intelligence and Informatics (CINTI), 2010 11th International Symposium on},
  DOI                      = {10.1109/CINTI.2010.5672233},
  File                     = {:../papers/wum-2015/Mining User Access Logs to Optimize Naviga- tional Structure of Adaptive Web Sites.pdf:PDF},
  Keywords                 = {Web sites, data mining, document handling, Web technique, adaptive Web site, navigation structure, navigational structure, shortcutting, user access log mining, user navigation path, visitor behavior, waypost document, Computational intelligence, Data mining, Data preprocessing, Informatics, Navigation, Optimization, rank1},
  Owner                    = {jaimeguzman},
  Timestamp                = {2015.07.27}
}

@Article{Feder1992,
  Title                    = {Universal prediction of individual sequences},
  Author                   = {Feder, M. and Merhav, N. and Gutman, M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1992},
  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {1258-1270},
  Volume                   = {38},
  Abstract                 = {The problem of predicting the next outcome of an individual binary sequence using finite memory is considered. The finite-state predictability of an infinite sequence is defined as the minimum fraction of prediction errors that can be made by any finite-state (FS) predictor. It is proven that this FS predictability can be achieved by universal sequential prediction schemes. An efficient prediction procedure based on the incremental parsing procedure of the Lempel-Ziv data compression algorithm is shown to achieve asymptotically the FS predictability. Some relations between compressibility and predictability are discussed, and the predictability is proposed as an additional measure of the complexity of a sequence},
  DOI                      = {10.1109/18.144706},
  ISSN                     = {0018-9448},
  Keywords                 = {binary sequences, data compression, filtering and prediction theory, Lempel-Ziv data compression algorithm, binary sequence, complexity, compressibility, finite memory, finite state predictor, finite-state predictability, incremental parsing procedure, individual sequences, infinite sequence, predictability, prediction errors, universal sequential prediction, Binary sequences, Data compression, Frequency, rank5}
}

@Article{Gopalratnam2007,
  Title                    = {Online Sequential Prediction via Incremental Parsing: The Active LeZi Algorithm},
  Author                   = {Gopalratnam, K. and Cook, D.J.},
  Journal                  = {Intelligent Systems, IEEE},
  Year                     = {2007},
  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {52-58},
  Volume                   = {22},
  Abstract                 = {Intelligent systems that can predict future events can make more reliable decisions. Active LeZi, a sequential prediction algorithm, can reason about the future in stochastic domains without domain-specific knowledge. In this article, potential of constructing a prediction algorithm based on data compression techniques are investigated. Active LeZi prediction algorithm approaches sequential prediction from an information-theoretic standpoint. For any sequence of events that can be modeled as a stochastic process, ALZ uses Markov models to optimally predict the next symbol},
  DOI                      = {10.1109/MIS.2007.15},
  ISSN                     = {1541-1672},
  Keywords                 = {Markov processes, data compression, prediction theory, text analysis, Active LeZi, Markov models, data compression, information theory, intelligent systems, sequential prediction algorithm, stochastic process, Compression algorithms, Data compression, Entropy, Frequency, Information theory, Intelligent systems, Prediction algorithms, Predictive models, Probability, Stochastic processes, Active LeZi, MavHome, sequential prediction, smart environments, rank2},
  Review                   = {Learning of sequential data continues to be a fundamental task and a challenge in pattern recognition and machine learning.}
}

@InCollection{Gueniche2015,
  Title                    = {CPT+: Decreasing the Time/Space Complexity of the Compact Prediction Tree},
  Author                   = {Gueniche, Ted and Fournier-Viger, Philippe and Raman, Rajeev and Tseng, VincentS.},
  Booktitle                = {Advances in Knowledge Discovery and Data Mining},
  Publisher                = {Springer International Publishing},
  Year                     = {2015},
  Editor                   = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
  Pages                    = {625-636},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {9078},
  DOI                      = {10.1007/978-3-319-18032-8_49},
  ISBN                     = {978-3-319-18031-1},
  Keywords                 = {Sequence prediction, Next item prediction, Accuracy, Compression, rank3},
  Language                 = {English},
  URL                      = {http://dx.doi.org/10.1007/978-3-319-18032-8_49}
}



@Article{hartmann2007,
  Title                    = {Prediction Algorithms for User Actions.},
  Author                   = {Hartmann, Melanie and Schreiber, Daniel},
  Year                     = {2007},
  Pages                    = {349--354},
  Booktitle                = {LWA},
  Keywords                 = {rank4}
}

@Article{Kewen2012,
  Title                    = {Analysis of Preprocessing Methods for Web Usage Data},
  Author                   = {Liu Kewen},
  Journal                  = {Intemational Conference on Measurement, Information and Control (MIC)},
  Year                     = {2012},
  File                     = {:../papers/wum-2015/Analysis of Preprocessing Methods for Web Usage Data.pdf:PDF},
  Owner                    = {jaimeguzman},
  Timestamp                = {2015.07.27}
}


@Article{Kotiyal2013,
  Title                    = {User behavior analysis in web log through comparative study of Eclat and Apriori},
  Author                   = {Kotiyal, Bina and Kumar, Ankit and Pant, Bhaskar and Goudar, R.H. and Chauhan, Shivali and Junee, Sonam},
  Year                     = {2013},
  Month                    = {Jan},
  Pages                    = {421-426},
  Abstract                 = {As we know World Wide Web plays vital role in serving the needs of the user's on web. The web log files are generated as a result of an interaction between the client and the service provider on web. Web log file contains the massive hidden valuable information pertaining to the visitors, if mined can be used for predicting the navigation behavior of the users. However the task of discovering frequent sequence patterns from the web log is challenging. Sequential pattern mining provides a significant role in serving a promising approach of the access behavior of the user. This paper focuses on adopting an intelligent technique that can provide personalized web service for accessing related web pages more efficiently and effectively, so that it can be determined which web pages are more likely to be accessed by the user in future. This paper uses two intelligent algorithms for predicting the user behavior's namely Apriori and Eclat and also does the performance comparison of the two algorithms in terms of time and space complexity for the filtered data.},
  Booktitle                = {Intelligent Systems and Control (ISCO), 2013 7th International Conference on},
  DOI                      = {10.1109/ISCO.2013.6481192},
  Keywords                 = {Pattern matching;Algorithm Comparison;Apriori;Eclat;Filtered Data;Proposed Architecture;Recommended System;Sequential Pattern Mining;Web Usage Mining}
}

@Article{Langdon1983,
  Title                    = {A note on the Ziv - Lempel model for compressing individual sequences (Corresp.)},
  Author                   = {Langdon, G.G., Jr.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1983},
  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {284-287},
  Volume                   = {29},
  Abstract                 = {The Ziv-Lempel compression algorithm is a string matching and parsing approach to data compression. The symbolwise equivalent for parsing models has been defined by Rissanen and Langdon and gives the same ideal codelength at the same cost in coding parameters. By describing the context and coding parameter for each symbol an insight is provided into how the Ziv-Lempel method achieves compression. This treatment does not employ a probabilistic source for the data string. The Ziv-Lempel method effectively counts symbol instances within parsed phrases. The coding parameter for each symbolwise context is determined by cumulative count ratios. The code string length increase for a symbol y following substring s , under the symbolwise equivalent, is the log of the ratio of node counts in subtrees s and s\cdot y of the Ziv-Lempel parsing tree. To demonstrate the symbolwise equivalent of the Ziv-Lempel algorithm, we extend the work of Rissanen and Langdon to incomplete parse trees. The result requires the proper handling of the comma when one phrase is the prefix of another phrase.},
  DOI                      = {10.1109/TIT.1983.1056645},
  ISSN                     = {0018-9448},
  Keywords                 = {Data compression, Encoding, Frequency, Instruments, Laplace equations, Phase change materials, Probability distribution, Rate distortion theory, Smoothing methods, Source coding, Speech, rank2},
  Owner                    = {jaimeguzman}
}

@Article{Li2005,
  Title                    = {R.: Genre classification via an lz78-based string kernel},
  Author                   = {Ming Li and Ronan Sleep},
  Year                     = {2005},
  Pages                    = {252--259},
  Booktitle                = {In: Proceedings of the 6th International Conference on Music Information Retrieval (ISMIR 2005},
  Keywords                 = {rank5}
}

@Article{Li2013,
  Title                    = {Research of Analysis of User Behavior Based on Web Log},
  Author                   = {Jia Li},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {601-604},
  Abstract                 = {This paper first introduces the theory and knowledge related to Web logs, and then introduces a Web log mining process. As to this article is based on Web log processing, we draw valid user data according to a specific website users' access data through the preprocessing, and then research and analyze the users behaviors. The work can provide a theoretical basis for the management and optimization of the site for site managers. Experiments have showed that the work is effective.},
  Booktitle                = {Computational and Information Sciences (ICCIS), 2013 Fifth International Conference on},
  DOI                      = {10.1109/ICCIS.2013.165},
  File                     = {:../papers/wum-2015/Jia Li - 06643080.pdf:PDF},
  Keywords                 = {Internet;data mining;Web log mining process;Website user access data;user behavior analysis;Data preprocessing;IP networks;Protocols;Web mining;Web servers;Web log;data mining;user behavior analysis},
  Owner                    = {jaimeguzman},
  Timestamp                = {2015.07.27}
}

@Article{Moghaddam2009,
  Title                    = {Dynamic and memory efficient web page prediction model using LZ78 and LZW algorithms},
  Author                   = {Moghaddam, A. and Kabir, E.},
  Year                     = {2009},
  Month                    = {Oct},
  Pages                    = {676-681},
  Abstract                 = {Web access prediction has attracted significant attention in recent years. Web prefetching and some personalization systems use prediction algorithms. Most current applications that predict the next user Web page have an offline component that does the data preparation task and an online section that provides personalized content to the users based on their current navigational activities. In this paper we present an online prediction model that does not have an offline component and fit in the memory with good prediction accuracy. Our algorithm is based on LZ78 and LZW algorithms that are adapted for modeling the user navigation in Web. Our model decreases computational complexities which is a serious problem in developing online prediction systems. A performance evaluation is presented using real Web logs. This evaluation shows that our model needs much less memory than PPM family of algorithms with good prediction accuracy.},
  Booktitle                = {Computer Conference, 2009. CSICC 2009. 14th International CSI},
  DOI                      = {10.1109/CSICC.2009.5349657},
  File                     = {:../docs/paper-tesis/Moghaddam_Kabir.pdf:PDF},
  Keywords                 = {Web sites, storage management, LZ78 algorithm, LZW algorithm, Web access prediction, Web logs, Web prefetching, World Wide Web, computational complexity, data preparation task, dynamic Web page prediction, memory efficient Web page prediction model, online prediction model, online prediction system, online section, performance evaluation, personalization system, prediction algorithm, uch less memory than PPM family of, user navigation, Accuracy, Association rules, Data mining, Entropy, Filtering, Navigation, Prediction algorithms, Predictive models, Prefetching, Web pages, LZ78, LZW, Web Page Prediction, rank5},
  Review                   = {Las Predicciones en los registros webaccess ha atraido mucha atencion en los últimos años

    La mayoria de las aplicaciones actuales que predicen el siguiente acceso a un página web posee una  componente offline 
    que hace la tarea de preparar la data y una sección en linea que permite personalizar cierto contenido para un usuario en particular
    basado en las actividades de navegación


    In this paper we present an online prediction model that does not have an offline component and fit in the memory with good prediction accuracy.


    In web page prediction understanding the user navigation pattern and then predicting the next pages is the main problem.



    In most of the web usage mining techniques, sequences are either used to produce association rules or to produce tree structures or Markov chains to represent navigation patterns. Markov models are based on a well-established theory and are simple to understand. They wildly used for modeling user navigation path.


    In recent years the content of many web sites are dynamic and new pages are also added to site dynamically. So we need a model that can be online and consider the changes of web site and user behavior. The memory efficiency is an important factor for an online algorithm. Most of the current models proposed for prediction are not online [8, 9] and need an offline component for processing tasks and online models such as model that proposed in [7] may soon become too big to fit in memory.


    We do not build per-user predictive models.

    LZ78 and LZW algorithms. These algorithms are compression algorithms and also are used in sequence mining.

     We use these algorithms for modeling the user navigation history.


    The techniques that rely on sequential patterns such as Markov models and sequential association rules mining contain more precise information about users’ navigation behavior. 

    There is an arc from node A to B if and only if at some point in time a client accessed to B within w accesses after A, where w is the lookahead window size. The weight of the arc is the ratio of the number of accesses to B within a window after A to the number of accesses to A itself. 


    In this section we propose two algorithms that are based on LZ78 and LZW. LZ78 algorithm is proposed by Jacob Ziv and Abraham Lempel in 1977 [15].


    LZ78 basically are lossless data compression algorithms with good functionality. The most important part of these algorithms is the dictionary construction algorithm that we use it for creating the prediction model.

    Usar el ejemplo de construccion del arbol de LZ para explicar en la tesis
    }
}

@Article{PenaSordo2015,
  Title                    = {Using compression models for filtering troll comments},
  Author                   = {de-la-Pena-Sordo, Jorge and Pastor-Lopez, Iker and Santos, Igor and Bringas, Pablo G.},
  Year                     = {2015},
  Month                    = {June},
  Pages                    = {655-660},
  Abstract                 = {Internet is evolving. How the content is generated has changed and currently, users and readers of a site can create content. They can express themselves showing their feelings or opinions commenting diverse stories or other users' comments in social news websites. This fact has led to negative side effects: the appearance of troll users and their contents seeking deliberately controversy. In this paper we propose a new method to filter trolling comments using compression models. Normally, Vector Space Model representation use is quite common but these filters can be attacked. To this end, we validate our approach with data from ‘Menéame’, a popular Spanish social news site, training several compression models, showing that our method can maintain high accuracy rates whilst making such filters difficult to defeat.},
  Booktitle                = {Industrial Electronics and Applications (ICIEA), 2015 IEEE 10th Conference on},
  DOI                      = {10.1109/ICIEA.2015.7334191},
  Keywords                 = {Algorithm design and analysis, Compression algorithms, Dictionaries, Government, Mathematical model, Probabilistic logic, Training, rank4}
}

@InProceedings{Poornalatha2012,
  Title                    = {Web Page Prediction by Clustering and Integrated Distance Measure},
  Author                   = {Poornalatha, G. and Raghavendra, P.S.},
  Booktitle                = {Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on},
  Year                     = {2012},
  Month                    = {Aug},
  Pages                    = {1349-1354},
  Abstract                 = {The tremendous progress of the internet and the World Wide Web in the recent era has emphasized the requirement for reducing the latency at the client or the user end. In general, caching and prefetching techniques are used to reduce the delay experienced by the user while waiting to get the web page from the remote web server. The present paper attempts to solve the problem of predicting the next page to be accessed by the user based on the mining of web server logs that maintains the information of users who access the web site. The prediction of next page to be visited by the user may be pre fetched by the browser which in turn reduces the latency for user. Thus analyzing user's past behavior to predict the future web pages to be navigated by the user is of great importance. The proposed model yields good prediction accuracy compared to the existing methods like Markov model, association rule, ANN etc.},
  DOI                      = {10.1109/ASONAM.2012.231},
  File                     = {:../papers/clustering/G. Poornalatha, Prakash S. Raghavendra - 06425570.pdf:PDF},
  Keywords                 = {Internet, data mining, pattern clustering, user interfaces, ANN, Markov model, Web page prediction, Web server, Web server log mining, World Wide Web, artificial neural network, association rule, caching technique, distance measure clustering, distance measure integration, prefetching technique, user behavior analysis, Accuracy, Browsers, Markov processes, Predictive models, Servers, Web pages, clustering, sequence alignment, user session, rank4}
}

@InCollection{Rabiner1990,
  Title                    = {Readings in Speech Recognition},
  Author                   = {Rabiner, Lawrence R.},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Year                     = {1990},
  Address                  = {San Francisco, CA, USA},
  Chapter                  = {A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
  Editor                   = {Waibel, Alex and Lee, Kai-Fu},
  Pages                    = {267--296},
  Acmid                    = {108253},
  ISBN                     = {1-55860-124-4},
  Numpages                 = {30},
  URL                      = {http://dl.acm.org/citation.cfm?id=108235.108253}
}

@Article{Rajimol2012,
  Title                    = {Web Access Pattern Mining, A Survey},
  Author                   = {A. Rajimol and G. Raju},
  Year                     = {2012},
  Abstract                 = {effective web site management, creating adaptive Web sites, business and support services, personalization and so on},
  Keywords                 = {WAP Tree, Web Access Pattern Mining, Pre-order Linked Web Access Pattern Mining, First-Occurrence Linked Pattern Tree mining, First-Occurrence Forest Mining, Conditional Sequence Mining.},
  Owner                    = {jaimeguzman},
  Timestamp                = {2015.07.27}
}

@Article{Rissanen1983,
  Title                    = {A universal data compression system},
  Author                   = {Rissanen, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1983},
  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {656-664},
  Volume                   = {29},
  Abstract                 = {A universal data compression algorithm is described which is capable of compressing long strings generated by a "finitely generated" source, with a near optimum per symbol length without prior knowledge of the source. This class of sources may be viewed as a generalization of Markov sources to random fields. Moreover, the algorithm does not require a working storage much larger than that needed to describe the source generating parameters.},
  DOI                      = {10.1109/TIT.1983.1056741},
  ISSN                     = {0018-9448},
  Keywords                 = {Source coding, Context modeling, Costs, Data compression, Encoding, Helium, Image coding, Image segmentation, Partitioning algorithms, Power generation, Statistics, rank2},
  URL                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1056741&queryText=A%20universal%20data%20compression%20system.%20IEEE%20Transactions%20on%20Information%20Theory,&newsearch=true}
}

@Article{Rissanen1984,
  Title                    = {Universal coding, information, prediction, and estimation},
  Author                   = {Rissanen, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1984},
  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {629-636},
  Volume                   = {30},
  Abstract                 = {A connection between universal codes and the problems of prediction and statistical estimation is established. A known lower bound for the mean length of universal codes is sharpened and generalized, and optimum universal codes constructed. The bound is defined to give the information in strings relative to the considered class of processes. The earlier derived minimum description length criterion for estimation of parameters, including their number, is given a fundamental information, theoretic justification by showing that its estimators achieve the information in the strings. It is also shown that one cannot do prediction in Gaussian autoregressive moving average (ARMA) processes below a bound, which is determined by the information in the data.},
  DOI                      = {10.1109/TIT.1984.1056936},
  ISSN                     = {0018-9448},
  Keywords                 = {Information theory, Parameter estimation, Prediction methods, Source coding, Additive noise, Additive white noise, Bandwidth, Broadcasting, Communication systems, Data compression, Feedback, Parameter estimation, Rate distortion theory, Viterbi algorithm, rank2},
  URL                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1056936&queryText=Universal%20coding,%20information,%20prediction,%20and%20estimation.&newsearch=true}
}

@Article{RissanenLangdon1979,
  Title                    = {Arithmetic Coding},
  Author                   = {Rissanen, J. and Langdon, G.G., Jr.},
  Journal                  = {IBM Journal of Research and Development},
  Year                     = {1979},
  Month                    = {March},
  Number                   = {2},
  Pages                    = {149-162},
  Volume                   = {23},
  Abstract                 = {The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated.},
  DOI                      = {10.1147/rd.232.0149},
  ISSN                     = {0018-8646},
  Keywords                 = {rank2},
  URL                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5390830&queryText=Arithmetic%20coding.%20IBM%20Journal%20of%20Research%20and%20Development&refinements=4230865550&searchField=Search_All}
}

@Article{Ryabko2002,
  Title                    = {On Asymptotically Optimal Methods of Prediction and Adaptive Coding for Markov Sources },
  Author                   = {Boris Ya. Ryabko and Flemming Topsøe},
  Journal                  = {Journal of Complexity },
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {224 - 241},
  Volume                   = {18},
  Abstract                 = {The problem of predicting a sequence x1, x2, … generated by a discrete source with unknown statistics is considered. Each letter xt+1 is predicted using information on the word x1x2…xt only. In fact, this problem is a classical problem which has received much attention. Its history can be traced back to Laplace. To estimate the efficiency of a method of prediction, three quantities are considered: the precision as given by the Kullback–Leibler divergence, the memory size of the program needed to implement the method on a computer, and the time required, measured by the number of binary operations needed at each time instant. A method is presented for which the memory size and the average time are close to the minimum. The results can readily be translated to results about adaptive coding. },
  DOI                      = {http://dx.doi.org/10.1006/jcom.2001.0611},
  ISSN                     = {0885-064X},
  Keywords                 = {prediction},
  URL                      = {http://www.sciencedirect.com/science/article/pii/S0885064X01906119}
}

@Article{Sculley2006,
  Title                    = {Compression and machine learning: a new perspective on feature space vectors},
  Author                   = {Sculley, D. and Brodley, C.E.},
  Year                     = {2006},
  Month                    = {March},
  Pages                    = {332-341},
  Abstract                 = {The use of compression algorithms in machine learning tasks such as clustering and classification has appeared in a variety of fields, sometimes with the promise of reducing problems of explicit feature selection. The theoretical justification for such methods has been founded on an upper bound on Kolmogorov complexity and an idealized information space. An alternate view shows compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. Thus, compression-based methods are not a "parameter free" magic bullet for feature selection and data representation, but are instead concrete similarity measures within defined feature spaces, and are therefore akin to explicit feature vector models used in standard machine learning algorithms. To underscore this point, we find theoretical and empirical connections between traditional machine learning vector models and compression, encouraging cross-fertilization in future work},
  Booktitle                = {Data Compression Conference, 2006. DCC 2006. Proceedings},
  DOI                      = {10.1109/DCC.2006.13},
  ISSN                     = {1068-0314},
  Keywords                 = {data compression, data structures, learning (artificial intelligence), Kolmogorov complexity, classification, clustering, compression, data representation, feature selection, feature space vectors, machine learning, Area measurement, Compression algorithms, Computer science, Concrete, Data compression, Extraterrestrial measurements, Machine learning, Machine learning algorithms, Measurement standards, Upper bound, rank5}
}

@Article{Shan2011,
  Title                    = {The research of web users' behavior mining based on association rules},
  Author                   = {Xiaohong Shan and Huamei Sun},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {7415-7418},
  Abstract                 = {When accessing websites, users usually leave a lot of access information, which can be mined reasonably to help the managers of website to get accessing patterns of users. This article first introduces the preprocessing procedure of web logs, which includes the tasks of data cleaning, Data Discretization and their implementation. On the basis of preprocessing the analysis method of requested resource “URL” and “referrer” which is the webpage before users browse the URL in web log by the use of association rules is proposed to find the accessing patterns of users. Finally the experiment is accomplished. The result shows that the method is feasible, and it can help the manager in making decisions about the analysis of website users' behavior and the optimization of website.},
  Booktitle                = {Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC), 2011 2nd International Conference on},
  DOI                      = {10.1109/AIMSEC.2011.6011219},
  Keywords                 = {Web sites;data mining;information retrieval;optimisation;user interfaces;URL;Web sites;Web users behavior mining;access information;association rules;data cleaning;data discretization;optimization;web log;Association rules;Data preprocessing;Google;Itemsets;Knowledge engineering;Software;Association Rules;Web Log;Web mining}
}

@InProceedings{Shkarin2002,
  Title                    = {PPM: one step to practicality},
  Author                   = {Shkarin, D.},
  Booktitle                = {Data Compression Conference, 2002. Proceedings. DCC 2002},
  Year                     = {2002},
  Pages                    = {202-211},
  Abstract                 = {PPM is one of the most promising lossless data compression algorithms using Markov source model of order D. Its main essence is the coding of a new (in the given context) symbol in one of inner nodes of the context tree; a sequence of the special escape symbols is used to describe this node. In reality, the majority of symbols is encoded in inner nodes and the Markov model becomes rather conventional. In spite of the fact that the PPM algorithm achieves the best results in comparison with others, it is used rarely in practical applications due to its high computational complexity. This paper is devoted to the PPM algorithm implementation that has a complexity comparable with widespread practical compression schemes based on LZ77, LZ78 and BWT algorithms. This scheme has been proposed by Shkarin (see Problems of Information Transmission, vol.34, no.3, p.44-54, 2001) and named PPM with information inheritance (PPMII).},
  DOI                      = {10.1109/DCC.2002.999958},
  ISSN                     = {1068-0314},
  Keywords                 = {Markov processes, computational complexity, data compression, encoding, prediction theory, probability, BWT algorithm, LZ77 algorithm, LZ78 algorithm, Markov model, Markov source model, PPM, PPM algorithm, PPMII, coding, context tree, escape symbols sequence, information inheritance, inner nodes, lossless data compression algorithms, prediction by partial matching, Chromium, Data compression, rank4}
}

@InProceedings{zaharia2010,
  Title                    = {Spark: cluster computing with working sets},
  Author                   = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  Booktitle                = {Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
  Year                     = {2010},
  Pages                    = {10},
  Volume                   = {10}
}


%%%%%%%%%% ALGORITMOS BASE DE LZ
@Article{ZivLempel1977,
  Title                    = {A universal algorithm for sequential data compression},
  Author                   = {Ziv, J. and Lempel, A.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1977},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {337-343},
  Volume                   = {23},
  Abstract                 = {A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.},
  DOI                      = {10.1109/TIT.1977.1055714},
  ISSN                     = {0018-9448},
  Keywords                 = {Sequential coding, Source coding, Books, Compression algorithms, Data compression, Data processing, Displays, Information theory, Jacobian matrices, Telephony, Testing, Upper bound, rank2}
}

@Article{ZivLempel1978, 
  Title           ={Compression of individual sequences via variable-rate coding},
  Author          ={J. Ziv and A. Lempel}, 
  Journal         ={IEEE Transactions on Information Theory}, 
  Year            ={1978}, 
  volume          ={24}, 
  number          ={5}, 
  pages           ={530-536}, 
  abstract        ={ Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity \rho(x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of \rho(x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences.}, 
  keywords  ={Source coding;Variable-rate coding;Algorithm design and analysis;Approximation algorithms;Digital signatures;Educational institutions;Entropy;Jacobian matrices;Laboratories;Notice of Violation;Public key cryptography;Time sharing computer systems}, 
  DOI       ={10.1109/TIT.1978.1055934}, 
  ISSN      ={0018-9448}, 
  Month     ={Sep},
}



@comment{jabref-meta: databaseType:bibtex;}

