\chapter[Experimental]{Experimental}





{
% Los objetivos no estan diferenciados entre generales y especıficos.
% Se presentan procesos que validan el trabajo comparando con otros metodos del estado del arte. 
% Se presenta una validacion deparametros,y todas las dimensiones que fueron planteadas son exploradas en profundidad.
% Se detalla y explica la recoleccion (de aplicar) y analisis de da- tos y resultados.
% Se distinguen los resultados segun las variables investigadas, y se examinan en virtud de lo explicado en el marco teorico.
%Se interpretan con claridad (es decir, se consideran distintos factores y anteceden- tes descritos hasta el momento) los resultados obtenidos.
}

Lo que se busca es un modelo de accesos de navegación secuencial creado por un algoritmo de compresión, Lempel Ziv,  para usarlo como un modelo  predicción. Se usará para buscar resultados sobre secuencias finitas discretas de \emph{webaccess logs}. Al momento de generar el árbol este creará una representación \emph{Trie} de un diccionario de símbolos, el cual utilizaremos para poder hacer predicciones.

En base a lo anterior y teniendo en funcionamiento el algoritmo para crear un modelo de predicción, se integrará con el servidor de Machine Learning Prediction.IO que ya se ha explicado anteriormente. Usaremos la misma API que ofrece Prediction.IO para poder realizar las evaluaciones de nuestras métricas propuestas, dado el dominio de nuestro problema la métrica a usar será Accurracy (Exactitud) frente a distintas porciones de datos. Con el dataset que tenemos haremos variadas pruebas para ver métricas como Accurracy, usaremos Cross Validation en distintos escenarios que nos permitirán tener información relevante de comportamiento de nuestro modelo propuesto en escenarios ideales como en escenarios reales.

%- Cross validation: This method is generally applied in machine-learning evaluation [14]. In our experiments, we performed a K-fold cross validation with k = 10. In this way, our dataset is 10 times split into 10 different sets of learning (90 
% of the total dataset) and testing (10 % of the total data).
%-Learning the model: We accomplished the learning step using different learning algorithms depending on





\section{Nuestro Modelo de Predicción ML-LDC}


%%%%%%% \cite{Moghaddam2009}

%The techniques that rely on sequential patterns such as Markov models and sequential association rules mining contain more precise information about users’ navigation behavior. 


%There is an arc from node A to B if and only if at some point in time a client accessed to B within w accesses after A, where w is the lookahead window size. The weight of the arc is the ratio of the number of accesses to B within a window after A to the number of accesses to A itself. 

Basados en un servidor de Machine Learning, hemos desarrollado un motor de predicción con un algoritmo basado en Lempel Ziv 78. Cada sesión es representada por un nodo y sus hijos. Sea la sesión \begin{equation}
\{ A,A,A,B,A,B,B,B,B,B,A,A,B,C,C,D,D,C,B,A,A,A,A \}
\end{equation}, la cual representaremos con nuestro modelo de predicción


	\begin{forest} 
	[ $\epsilon$
		[A
			[AA
				[AAA]	
			]
			[AB
				[ABC]
			]
		]
		[B
			[BA]
			[BB
				[BBA]
			]
		]
		[C]
		[D
			[DC]
		]
	]
	\end{forest}

Dado nuestro modelo de predicción podemos determinar que nuestra salida resultante será:

\begin{equation}
\{ A,AA,B,AB,BB,BBA,ABC,C,D,DC,BA,AAA \}
\end{equation}


Acotamos los siguientes casos en que nuestro Modelo de predicción funcionará


\begin{enumerate}
	\item \textbf{Nodos Intermedios probabilidad Equivalente}
	
	Dada la $P( x | A\epsilon  )$, siendo $x$ la probabilidad de encontrar el siguiente símbolo. Este caso nos muestra que tenemos dos secuencias posibles $\{A,AB\}$, por lo cual podemos hacer la función $arc_max(AA,AB)$ ó calcular una función $random(AA,AB)$, al ser un caso con solamente dos posibilidades tenemos solo un $50\%$ de éxito.
	
	
	\item \textbf{Nodos intermedios con un nodo hijo }
	
	Dada la $P( x | AB  )  = ABC$, ya que  el nodo $C$, es el único hijo por tanto tiene toda la certeza de ser la predicción acorde al árbol de entrenamiento.
		
		
		
	\item \textbf{Nodo hoja y vuelta a la raíz}	
	
	Este caso es uno de los más sencillos ya que nuestro modelo al no tener más secuencias para poder hacer las operaciones, se proyectará el árbol para todos los posibles accesos dentro de nuestro alfabeto. Para el caso $\Sigma = \{A,B,C,D \} $ y la probabilidad para cada acceso representado por símbolos es de $25\%$.
	
	
	Dada la probabilidad $P( x ) $ al momento de retornar a la raíz, la siguiente secuencia es absorbida por $\epsilon$.
	

\end{enumerate}



\section{Ambientes Experimental}

Para los ambientes experimentales se han dispuestos dos máquinas para realizar las pruebas. 

\subsubsection{Máquinas}
	\begin{itemize}
		\item Procesador 2,8 GHz Intel Core i7, 16 GB de Memoria RAM y Sistema Operativo OSX
		\item Procesadores Intel Xeon E5-2670 v2 (Ivy Bridge) de alta frecuencia 32 vCPU, 244 GB de Memoria RAM y Sistema Operativo Ubuntu 14.14 
	\end{itemize}
	
Para el proceso de desarrollo con IntelliJ y dataset menores a 500 sesiones se usará la máquina con 16GB y para experimentos con sesiones mayores a 1000 sesiones de usuarios se usará la máquina 240GB.	
	

\subsubsection{Software}

	\begin{itemize}
		\item C++11
		\item Java  1.8
		\item Java(TM) SE Runtime Environment (build 1.8)
		\item Java HotSpot(TM) 64-Bit Server VM (build $25.51-b03$, mixed mode)
		\item Scala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL
		\item SBT 0.13.9 
		\item Python 2.7.10
		\item GNU bash 3.2.57 $(x86_64-apple-darwin15)$
		\item Prediction.IO 0.9.4
		\item Elasticsearch 1.4.4	
		\item Apache Spark-1.4.1
		\item Hbase 1.0.0
		\item Zookeeper 
	\end{itemize}



% Selección de la métrica

% https://docs.prediction.io/evaluation/metricchoose/

% Explicación de evaluación

% https://docs.prediction.io/templates/recommendation/evaluation/#evaluation-data-generation




El motor de predicción es Prediction.IO, como se ha señalado anteriormente  es un framework para desplegar servidores con algoritmos de Maquinas de Aprendizaje, Decission Tree, K-Means, RNN y todos los algoritmos ofrecidos por la suite de Apache Spark y MLib. % Colocar unos lindos links para hacer publicidad a esto

Para desarrollar un motor que se acople con \emph{PIO} se deberá seguir el patrón \emph{DASE} y crear un modelo con persistencia en memoria RAM que nos permita un acceso rápido a las predicciones por consultar.


Usaremos \emph{SBT} para gestionar todas las librerías que se requieran como dependencia. Inherentemente usaremos Java, ya que el lenguaje Scala corre sobre la Java Virtual Machine. Prediction.IO no solo ocupa Scala, adicionalmente provee el uso de Apache Spark con sus librerías de MLIB (Machine Learning Library), Zookeeper, Hbase (Hadoop) y Elasticsearch. Hemos utilizado Python para realizar un cliente por línea de comando en el cual poder hacer pruebas y adicionalmente incluir un cliente que realice la carga de eventos desde nuestro set de datos experimental. Para mayor referencia de los clientes python ver los fuentes en los anexos.





% Es una métrica que captura una porición correcta de toda la data a probar. 
% Una manera de modelar esto es para cada consulta que hacemos al predictor, damos un score de 1 y 0, luego podemos tomar el promedio de este score.

% PredictionIO sobre métricas


% PredictionIO has a [[AverageMetric]] helper class which provides this feature. This class takes 4 type parameters, [[EvalInfo]], [[Query]], [[PredictedResult]], and [[ActualResult]], these types can be found from the engine's signature. Line 5 below is the custom calculation.

%Precision is a metric for binary classifier capturing the portion of correction prediction among all positive predictions. We don't care about the cases where the QPA-tuple gives a negative prediction. (Recall that a binary classifier only provide two output values: positive and negative.) The following table illustrates all four cases:

%Calculating the precision metric is a slightly more involved procedure than calculating the accuracy metric as we have to specially handle the don't care negative cases.

%x PredictionIO provides a helper class OptionAverageMetric allows user to specify don't care values as None. It only aggregates the non-None values. Lines 3 to 4 is the method signature of calcuate method. The key difference is that the return value is a Option[Double], in contrast to Double for AverageMetric. This class only computes the average of Some(.) results. Lines 5 to 13 are the actual logic. The first if factors out the positively predicted case, and the computation is simliar to the accuracy metric. The negatively predicted case are the don't cares, which we return None.

% \subsection{Experimento comparativo con Frequent Sequency Pattern}


%\subsection{Decission Tree}
%\subsection{Asociation Rulz}





\section{Datos Experimentales}
% Cita a Rkonow Claude & Navarro

Se usarán las secuencias disponibles de \emph{webaccess logs} pública del sitio MSNBC. El set de datos provienen de los registros de un servidor IIS (Internet Information Services) msnbc.com de un día completo de la fecha  28 de Septiembre de 1999. 
Contiene secuencias de acceso web de 989,818 usuarios con un promedio de 5,7  categorías Página web visitas por secuencia, el tamaño de letras de este conjunto de datos es $\sigma \ = 17$.


Ejemplo de estructura de Dataset MSNBC :
\vspace{0.5cm}

\begin{lstlisting}[frame=single,basicstyle=\ttfamily\tiny,]
% Different categories found in input file:

frontpage news tech local opinion on-air misc weather msn-news health living business msn-sports sports summary bbs travel

% Sequences:

A A 
B 
C B B D B B B C C 
A A 
F 
F G G G F F H H H H 
F I D D D J C J E J D D D 
A A A K A A A 
L L 
A A 
H H H H H H 
\end{lstlisting}





Utilizaremos un simple programa en C++ para filtrar las sesiones de los usuarios, creando nuevos set de datos y transformarlos a símbolos, caracteres en mayúscula, por ejemplo: 
\footnote{Este programa se encuentra junto en el anexo de la memoria}

	
	\begin{lstlisting}[frame=single,basicstyle=\ttfamily\tiny,]
	1 1 
	2 
	3 2 2 4 2 2 2 3 3 
	5 
	1 
	6 
	1 1 
	6 
	6 7 7 7 6 6 8 8 8 8 
	6 9 4 4 4 10 3 10 5 10 4 4 4 
	\end{lstlisting}


 Ejemplo de sub-dataset creados:

%@TODO: Ver como hacer mas pequeño el dataset.
\begin{itemize}
	\item Secuencias de webaccess con sesiones de con un umbral mínimo.
	\item Secuencias de webaccess con sesiones de largo equivalente.
	\item Secuencias de webaccess con sesiones de más de 100 secciones web visitadas.
	\item Secuencias de webaccess con sesiones acotadas superiormente.
	
\end{itemize}


Trabajaremos con 1.000.000 de registros de los cuales hemos realizados distintos subconjuntos para hacer una validación cruzada:

\begin{itemize}
	\item 10 Sesiones de usuarios
	\item 100 Sesiones de usuarios
%	\item 500 Sesiones de usuarios
	\item 1.000 Sesiones de usuarios
%	\item 5.000 Sesiones de usuarios
	\item 10.000 Sesiones de usuarios
	\item 50.000 Sesiones de usuarios
	\item 100.000 Sesiones de usuarios
	\item 500.000 Sesiones de usuarios
%	\item 750.000 Sesiones de usuarios
	\item 1.000.000 Sesiones de usuarios
\end{itemize}


Es importante señalar que la división de nuestro set de datos es debido a que no conocemos la naturaleza de los mismo, pero con nuestro modelo podemos hacer un estudio para ir detectando patrones que puedan ser claves para medir el rendimiento en función a los criterios de volumen de datos versus el Accurracy(Exactitud).






\begin{table}[]
	\centering

	\label{table-list-symbol}
	\begin{tabular}{cl}
		Page       & Symbol \\ \cline{1-2}
		frontpage  & A      \\
		news       & B      \\
		tech       & C      \\
		local      & D      \\
		opinion    & E      \\
		on-air     & F      \\
		misc       & G      \\
		weather    & H      \\
		msn-news   & I      \\
		health     & J      \\
		living     & K      \\
		business   & L      \\
		msn-sports & M      \\
		sports     & N      \\
		summary    & O      \\
		bbs        & P      \\
		travel     & Q      \\ 
	\end{tabular}
%	\caption{Para poder interpretar la entrada anterior se debe tener esta relación de símbolos con respecto a las páginas}
\end{table}





\section{Experimentos}









Dentro de nuestro experimentos para probar la exactitud de nuestro modelo podemos demostrar que al tener una menor cantidad de secciones visitadas, la probabilidad de no acertar crece considerablemente.


Sea la siguiente sesión de un usuario 
$\{A , B , A  \}  $ para un alfabeto $\Sigma=\{A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q \} $


	\begin{tikzpicture}[level/.style={sibling distance=40mm/#1}]
	\node [circle,draw] (epsilon){$\varepsilon$}
		child { node [circle,draw] (A) {$A$} 
				child { node[circle,draw](AA){$AA$}  }
			}
		child {node [circle,draw] (B) {$B$} }
	;
	
	\end{tikzpicture}


Como usted puede ver a menor cantidad de símbolos en la sesión estos generan un árbol de menor altura y menos nodos, cada nodo como se ha señalado en el capitulo 4 representa un visita a una sección en particular de la web de MSNBC. Cuando un webaccess posee  pocas secciones o páginas visitadas, la proyección de probabilidades de los posibles símbolos en el alfabeto hace que la probabilidad del siguiente acceso sea equiprobable dentro del los símbolos de nuestro diccionario, de lo anterior convergemos en que mayor es el entrenamiento mejor será la predicción.

Dado el evento $x$  a predecir que pertenece a una secuencia discreta, la probabilidad de $P( x| AB  ) = A $, es el resultado de esta sesión de entrenamiento, pero la probabilidad $P(x | AAB) = ?$, es desconocida.  Si extendemos los símbolos de este nodo cada nodo hijo tendría un probabilidad de $ \dfrac{1}{\Sigma} = \dfrac{1}{17} = 0.0588 $ o bien sería  $\dfrac{1}{ |\sigma| }$, siendo $|\sigma|$ el total de símbolos que se encuentran en el alfabeto. 

Para corroborar este comportamiento haremos distintos experimentos en distintos volúmenes de datos, usaremos una validación cruzada para medir el Accurracy, la cual será nuestra métrica a utilizar.

Con esto demostraremos que secuencias con menores cantidad de símbolos generar un tipo de ruido a la métrica  que afecta en su a nuestra exactitud esperada, cuando hacemos iteraciones de evaluaciones sobre el mayor orden, por otra parte veremos como con una menor cantidad de sesiones de entrenamiento podemos lograr un Accurracy bastante optimista.



\vspace{1cm}
\begin{enumerate}
	% Idea de experimento con disminución del tamaño del alfabeto
	\item \label{exp1} \textbf{Experimento con sesiones de usuarios con datos generados de forma sintética}
	
	
	Creamos un set de datos en el cual pudiesemos esperar valores conocidos. Además acotamos a un diccionario de solo tres elementos, el Accurracy obtenido es:
	
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Numero de Sesiones,
	ylabel=Accurracy en \% ]
	\addplot[color=black,mark=x] coordinates {
		(10, 66.6666666666666)
		(20, 73.6842105263157)
		(30, 75.8620689655172)
		(40, 76.9230769230769)
		(50, 73.469387755102)
		(60, 74.5762711864406)
		(70, 75.3623188405797)
	};
	\end{axis}
	\end{tikzpicture}
	
 
	
	En el gráfico anterior usamos como mínimo 10 sesiones de las 80 de disponibles que se usaron par este experimento. Dado a que en este caso la cantidad de sesiones es bien reducida y cada símbolo posee una gran frecuencia existe mayor redundancia de datos y nuestro modelo se comporta como espera que lo haga un algoritmo de compresión de datos. Entre mayor es la cantidad de símbolos iguales
	que van entrenando al \emph{LZtrie}, hay una aglomeración de frecuencia en ciertos nodos, pero estos son minimizados por los niveles que genera al momento de la construcción del árbol.
	
	La tasa de frecuencia de un símbolo converge a predicciones de secuencias evaluadas que caen en el nodo con mejor probabilidad dado $\epsilon$ (Raíz del \emph{trie}).
	


	\newcommand{\slice}[4]{
		\pgfmathparse{0.5*#1+0.5*#2}
		\let\midangle\pgfmathresult
		
		% slice
		\draw[thick,fill=black!10] (0,0) -- (#1:1) arc (#1:#2:1) -- cycle;
		
		% outer label
		\node[label=\midangle:#4] at (\midangle:1) {};
		
		% inner label
		\pgfmathparse{min((#2-#1-10)/110*(-0.3),0)}
		\let\temp\pgfmathresult
		\pgfmathparse{max(\temp,-0.5) + 0.8}
		\let\innerpos\pgfmathresult
		\node at (\midangle:\innerpos) {#3};
	}
	
	\begin{center}
	\begin{tikzpicture}[scale=1.5]
	
	\newcounter{a}
	\newcounter{b}
	\foreach \p/\t in {61.5/page A, 35.9/page B, 12.8/type C }
	{
		\setcounter{a}{\value{b}}
		\addtocounter{b}{\p}
		\slice{\thea/100*360}
		{\theb/100*360}
		{\p\%}{\t}
	}
	
	\end{tikzpicture}
	\end{center}
	

	Tal como se puede ver en el gráfico anterior existe un gran probabilidad de que dado una secuencia de accesos después de $\epsilon$ la próxima sección a acceder sea la página A.
	Esto es adicionalmente es consistente a el tipo de set de datos que hemos ocupado ya que con esto podemos delimitar a que nuestro modelo al tener símbolos bastante frecuente no crea un trie desbalanceado, para este caso solo acota constantemente a un altura de 5 niveles y una variación mínima entre la exactitud, que posee el entrenamiento versus set de evaluaciones, incluso podemos solo podemos usar un entrenamiento de por lo menos 30 sesiones para predecir 50 sesiones con un Accurracy con un margen de error máximo de $10\%$ en el peor de los casos. 
	Lo anterior si lo comparamos con un evento aleatorio como resultado nos daría que nuestro modelo es bastante mejor que una predicción aleatoria, es decir $ 66.7\%  \geq 33\%$, siendo esta una comparativa optimista de que al menos nuestro modelo dado un set de datos artificial.
	
	
	
	
	\newpage
	\item \label{exp2}
	\textbf{Sesiones con menor redundancia y secuencias de largo variable }
	

	Validaremos ahora el comportamiento de nuestro modelo con datos reales con secuencias discretas distribuidas no uniformemente.
	
	
	\begin{tikzpicture}
		\begin{axis}[
		xlabel= Numero de Sesiones,
		ylabel=Accurracy en \% ]
		\addplot[color=black,mark=x] coordinates {
			(10, 22.2471910112359)
			(20, 16.7088607594936)
			(30, 27.6328502415459)
			(40, 21.6949152542372)
			(50, 25.469387755102)
			(60, 24.7435897435897)
			(70, 25.5665024630541)
			(80, 26.3157894736842)
			(90, 33.3333333333333)
		};
		\end{axis}
	\end{tikzpicture}
		
	En este caso si existe una menor redundancia a diferencia del experimento \ref{exp1} lo que produce que el modelo tengo un rendimiento bajo, aún así dada sigue siendo en el mejor de los casos seis veces mejor que la probabilidad aleatoria de predecir. En este experimento usamos un $|\sigma| =15$, por ende tenemos que dado nuestro modelo,
	\begin{equation}\label{expResult2}
		M( x | \mbox{90\% train}  ) = 33 \% \geq M( x | \mbox{random}  ) = 6.66\% 
	\end{equation}, como hemos visto en (\ref{expResult2}) nuestro modelo sigue siendo válido en un escenario en que los datos se dispersan considerablemente. 
	Adicionalmente en este tipo de caso existe un comportamiento de nuestro modelo en el cual hace el mejor esfuerzo por mantener la compresibilidad de los datos mayor o igual a la predictibilidad del mismo, peor al tener mayor dispersión la altura para una cantidad similar de sesiones en (\label{expResult2}) sigue siendo 5. Pero la cantidad de nodos sufre un gran incremento, podemos verlo en:
	
	
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel=$\mbox{Sesiones de Entrenamiento}$,
	ylabel=$\mbox{Acurracy en \%}$]
	\addplot[smooth,mark=*,black] plot coordinates {
			(10, 22.2471910112359)
			(20, 16.7088607594936)
			(30, 27.6328502415459)
			(40, 21.6949152542372)
			(50, 25.469387755102)
			(60, 24.7435897435897)
			(70, 25.5665024630541)
			(80, 26.3157894736842)
			(90, 33.3333333333333)
	};

	
%	\addplot[smooth,color=red,mark=x]
%	plot coordinates {
%			(10, 10)
%			(20, 10)
%			(30, 10)
%			(40, 10)
%			(50, 19)
%			(60, 33)
%			(70, 56)
%			(80, 66)
%			(90, 69)
%	};
%	\addlegendentry{Nodes/train}
	\end{axis}
	\end{tikzpicture}


	Acorde al gráfico anterior podemos señalar que al haber un incremento en la dispersión de los datos y menor redundancia, el crecimiento de nuestro árbol será horizontal, ya que para tanto como hemos en este experimento y en (\label{expResult2}), la altura del nodo no tiene una relación directa a la redundancia ó dispersión, pero si a la cantidad de sesiones evaluadas. 
	
	Además, al mayor esfuerzo que hace el predictor por lograr mejores resultados se construyen mas nodos, por lo que el modelo LDC, deja de comprimir por satisfacer las condiciones de predictibilidad necesarias para seguir siendo válido.

	
	Iteramos una nueva evaluación con las mismas condiciones pero subiendo el volumen de datos de 100 a 1000. Con este buscaremos ver encontrar el mismo comportamiento a un mayor nivel de datos.
	


	\begin{tikzpicture}
		\begin{axis}[
		xlabel=$\mbox{Sesiones de Entrenamiento}$,
		ylabel=$\mbox{Acurracy en \%}$]
		\addplot[smooth,mark=*,black] plot coordinates {
			(100,  20.3715239154616)
			(200,  21.2302878598247)
			(300,  22.6490224129709)
			(400,  24.3752086811352)
			(500,  26.2308617234469)
			(600,  25.8692564745196)
			(700,  28.8423315814619)
			(800,  28.929431438127)
		};
		\end{axis}
	\end{tikzpicture}
	
	El modelo sigue siendo válido al ir aumentando el volumen de datos bajo las mismas circunstancias. Incluso para ambos experimentos anteriores podemos sacar la relación que nuestro modelo posee una tasa de error de $\pm4\%$ al aumentar 100 veces con respecto a la primera iteración de este escenario cuando se usa la  mayor cantidad de sesiones de entrenamiento posible.


	Otra particularidad que ha demostrado el modelo gracias a las propiedades de compresibilidad de Lempel Ziv\cite{ZivLempel1977} es la Minimización de niveles requeridos aún cuando la cantidad de nodos va creciendo.
	
	
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel=$\mbox{Niveles del Trie}$,
	ylabel=$\mbox{Número de nodos}$]
	\addplot[smooth,mark=*,blue] plot coordinates {
		( 10,5 )
		(  28 ,5)
		( 36 , 16)

	};

	\addlegendentry{ 1000 sesiones }

	\addplot[smooth,mark=*,red] plot coordinates {
		( 10,5 )
		( 10,5 )
		( 10,5 )
	};
	\addlegendentry{ 100 sesiones }
	
	\end{axis}
	\end{tikzpicture}


	Como podemos ver en el comportamiento de inicio del modelo el criterio de partida en el escenario descrito es compartido por ambos experimentos.

	También dado a que este no es uno de los casos más favorables para nuestro modelo de predictivo, podríamos tener un diferencial en los tiempos de construcción del Trienode acorde a la cantidad de nodos necesarios para los criterios de entrenamiento, 


	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Sesiones de Entrenamiento ,
	ylabel=Tiempo en \emph{seg} ]
	\addplot[color=black,mark=x] coordinates {
			(100, 45 )
			(200, 87 )
			(300, 94 )
			(400, 75 )
			(500, 84 )
			(600, 76 )
			(700, 67)
			(800, 72 )
	};
	\end{axis}
	\end{tikzpicture}
	
	Como podemos ver en el gráfico anterior podemos buscar un dataset optimo el cual puede encontrarse dentro del intervalo $ [ 300,400 ]$ sesiones de usuario, equivalente a la mejores resultados de \emph{Accurracy} y menor cantidad de secuencias de entrenamiento


	\item \label{exp4}	
	\textbf{Sesiones con tamaño de secuencia constante}

	En este experimentos queremos lograr el mismo comportamiento que tuvimos en el experimento [\ref{exp1}]. Haciendo un filtrado simple de los \emph{webaccess log } que hemos estado analizando podemos llegar a mejores valores que las predicciones de resultado aleatorio.


	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Numero de Sesiones de entrenamiento,
	ylabel=Accurracy en \% ]
	\addplot[color=black,mark=x] coordinates {
			(100, 39.3259176863181 )
			(200, 41.8698372966207 )
			(300, 42.1454458750596 )
			(400, 42.6323038397328 )
			(500, 41.9799599198396 )
			(600, 45.3634085213033 )
			(700, 44.7892976588628)
			(800, 43.9736180904522 )
			(900, 42.9539842873176 )
	};
	\end{axis}
	\end{tikzpicture}
	
	La cantidad total de sesiones usadas fueron 1000 y las cuales como en el gráfico anterior se señala a mayor cantidad entrenamiento existe al menos un punto de la curva que se vuelve un máximo.


	Seguido a esto podemos ver el tiempo de construcción del \emph{LZTrie} que nuestro modelo demora en generar:
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Sesiones de Entrenamiento ,
	ylabel=Tiempo en \emph{seg} ]
	\addplot[color=black,mark=x] coordinates {
			(100, 20)
			(200, 35)
			(300, 44)
			(400, 49)
			(500, 60)
			(600, 65)
			(700, 58)
			(800, 62)
			(900, 57 )
	};
	\end{axis}
	\end{tikzpicture}

	Al igual que en el gráfico anterior podemos ver que existe al menos una mínima cantidad sesiones las cuales generar un rendimiento sobre las evaluaciones a realizar. 








	\item \label{exp5}
	\textbf{Secuencias de accesos con limite inferior 100 símbolos }
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Numero de Sesiones,
	ylabel=Accurracy en \% ]
	\addplot[color=black,mark=x] coordinates {
		(10 ,  42.5190839694656 )
		(50 ,  44.595041322314 )
 		(100 , 49.089861751152 )
 		(200 , 56.7230538922155 )
 		(300 , 60.6837606837608 )
 		(400 , 64.9253731343282 )
 		(520 , 71.4285714285715 )			
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Sesiones de Entrenamiento ,
	ylabel=Tiempo en \emph{seg} ]
	\addplot[color=black,mark=x] coordinates {
		(10 ,  18.4)
		(50 ,  41.5)
		(100 , 69.46)
		(200 , 110)
		(300 , 134)
		(400 , 213)
		(520 , 222)	
	};
	\end{axis}
	\end{tikzpicture}

	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Cantidad de Nodos,
	ylabel=Sesiones de Entrenamiento  ]
	\addplot[color=black,mark=x] coordinates {
		(1012,10 )
		(1428,50 )
		(2481,100 )
		(3288,200 )
		(3919,300 )
		(4683,400 )
		(5038,520 )	
	};
	
 	
	\end{axis}
	\end{tikzpicture}	


	\begin{tikzpicture}
	\begin{axis}[
	xlabel=Cantidad de Nodos para LZTrie predictivo,
	ylabel=Accurracy en \%  ]
 
	\addplot[color=black,mark=x] coordinates {
		(1012,42.5190839694656 )
		(1428,44.595041322314 )
		(2481,49.089861751152 )
		(3288,56.7230538922155 )
		(3919,60.6837606837608 )
		(4683,64.9253731343282 )
		(5038,71.4285714285715 )	
	};	
	\end{axis}
	\end{tikzpicture}

	
	
	
	
	\item \label{exp6}	
	\textbf{Sesiones con menos de 5 webaccess para generar el \emph{LZTrie}}
	
	
	
	Hacemos una validación cruzada para una muestra de data de 906130 sesiones de usuarios para probar como se comporta con cross validation.
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel= Numero de Sesiones de Entrenamiento,
	ylabel=Accurracy en \% ]
	\addplot[color=red,mark=x] coordinates {
		(100, 29.9674371114825)
		(200, 44.9237667712101)
		(1000, 56.519572924979)
		(2500, 68.9237667712101)
 
	};
	\end{axis}
	\end{tikzpicture}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\item  Dataset uniformes de largo acotado inferiormente y volumenes de alto tamaño.


	
\end{enumerate}








 



	% \begin{forest} 
	% 	[VP
	% 	[DP]
	% 	[V’
	% 	[V]
	% 	[DP]
	% 	]
	% 	]
	% \end{forest}



	\begin{tikzpicture}[level/.style={sibling distance=60mm/#1}]
	\node [circle,draw] (z){$n$}
	child {node [circle,draw] (a) {$\frac{n}{2}$}
		child {node [circle,draw] (b) {$\frac{n}{2^2}$}
			child {node {$\vdots$}
				child {node [circle,draw] (d) {$\frac{n}{2^k}$}}
				child {node [circle,draw] (e) {$\frac{n}{2^k}$}}
			} 
			child {node {$\vdots$}}
		}
		child {node [circle,draw] (g) {$\frac{n}{2^2}$}
			child {node {$\vdots$}}
			child {node {$\vdots$}}
		}
	}
	child {node [circle,draw] (j) {$\frac{n}{2}$}
		child {node [circle,draw] (k) {$\frac{n}{2^2}$}
			child {node {$\vdots$}}
			child {node {$\vdots$}}
		}
		child {node [circle,draw] (l) {$\frac{n}{2^2}$}
			child {node {$\vdots$}}
			child {node (c){$\vdots$}
				child {node [circle,draw] (o) {$\frac{n}{2^k}$}}
				child {node [circle,draw] (p) {$\frac{n}{2^k}$}
					child [grow=right] {node (q) {$=$} edge from parent[draw=none]
						child [grow=right] {node (q) {$O_{k = \lg n}(n)$} edge from parent[draw=none]
							child [grow=up] {node (r) {$\vdots$} edge from parent[draw=none]
								child [grow=up] {node (s) {$O_2(n)$} edge from parent[draw=none]
									child [grow=up] {node (t) {$O_1(n)$} edge from parent[draw=none]
										child [grow=up] {node (u) {$O_0(n)$} edge from parent[draw=none]}
									}
								}
							}
							child [grow=down] {node (v) {$O(n \cdot \lg n)$}edge from parent[draw=none]}
						}
					}
				}
			}
		}
	};
	\path (a) -- (j) node [midway] {+};
	\path (b) -- (g) node [midway] {+};
	\path (k) -- (l) node [midway] {+};
	\path (k) -- (g) node [midway] {+};
	\path (d) -- (e) node [midway] {+};
	\path (o) -- (p) node [midway] {+};
	\path (o) -- (e) node (x) [midway] {$\cdots$}
	child [grow=down] {
		node (y) {$O\left(\displaystyle\sum_{i = 0}^k 2^i \cdot \frac{n}{2^i}\right)$}
		edge from parent[draw=none]
	};
	\path (q) -- (r) node [midway] {+};
	\path (s) -- (r) node [midway] {+};
	\path (s) -- (t) node [midway] {+};
	\path (s) -- (l) node [midway] {=};
	\path (t) -- (u) node [midway] {+};
	\path (z) -- (u) node [midway] {=};
	\path (j) -- (t) node [midway] {=};
	\path (y) -- (x) node [midway] {$\Downarrow$};
	\path (v) -- (y)
	node (w) [midway] {$O\left(\displaystyle\sum_{i = 0}^k n\right) = O(k \cdot n)$};
	\path (q) -- (v) node [midway] {=};
	\path (e) -- (x) node [midway] {+};
	\path (o) -- (x) node [midway] {+};
	\path (y) -- (w) node [midway] {$=$};
	\path (v) -- (w) node [midway] {$\Leftrightarrow$};
	\path (r) -- (c) node [midway] {$\cdots$};
	\end{tikzpicture}



%Las conclusiones son deducidas logicamen- te de los resultados obtenidos y de la interpretacionpresen- tada, ademas estan conectadas al marco teorico.
%Las conclusiones muestran el logro de los ob jetivos.
%Se presentan proyec- ciones validas y valio- sas a partir del traba- jo realizado.
%Se detallan claramen- te las limitaciones del traba jo realizado.



\subsection{Detección de Ruido en secuencias de acceso}

Al modelar una navegación de usuario mediante un trie basado en un algoritmo como LZ78, adoptamos un enfoque basado en la frecuencia por lo cual si realizamos experimentos para poder encontrar ruido veremos que que son secuencias de acceso comunes y estas no dan relevancia o aportan a la exactitud ó precisión del algoritmo.

Sea la secuencia $\{A,A,A,A,A,A,A,A,A,A,A,A \}$ a la cual llamaremos secuencia $R$, si $A$ es representado por el \emph{home} ó página de inicio, esto da a interpretación que existe un usuario en su sesión $R$ que se encuentra accediendo constantemente a esta sección. Podemos señalar que esta es una sesión ruidosa si:

\begin{equation}
	P( x | AAAAAAA)= A	
\end{equation}, pero siendo la sesión $R$ de tamaño = 12, en el siguiente acceso tendremos una probabilidad equiprobable dentro de las secciones en nuestro alfabeto, la cual generará un probabilidad de éxito ''falso positivo''.

En el siguiente experimento veremos como se comporta nuestro modelo dado entradas \emph{Ruidosas}. Además se mostrará como el árbol suele perder su balance a medida que va creciendo los niveles de altura. 



Por otro lado teniendo la noción de como es el funcionamiento de un servidor IIS, y al ser una página con un gran número de visitas, podemos señalar que los datos proporcionados no son totalmente representativos de usuarios reales, ya que la web al bien indexada en los buscadores existen un cantidad indeterminada de \emph{Crawlers} ó \emph{Bots} que están constantemente generando accesos tanto para almacenar en caché páginas o generando accesos automatizados a ciertos secciones sin ser datos representativo. Dejamos como discusión que algoritmo se podría implementar para detección de estos patrones por las ramas que hemos generado para la detección de \emph{bots} o \emph{robots}



 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APUNTES
%%%%%%%%  haciendo prubeas con crosss validate con  80 t y 20 de eval, consigo peor predicción que con menos data 
%%%%%%%%  Además el arbol  al tener mmás információn, comienza a crecer desmensuaradmente
%%%%%%%% de hecho las mejores predicciones son cunado hay ruido o sesiones repetitivas DATASOURCE >>>penultimate page= A  last page= A  session= AAAAAAAA
%%%%%%%%

 


%%%%%%%%

% 100 datos
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 28.3% con 10t y 90 eval
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 19.2% con 20t y 80 eval
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 18.4% con 30t y 70 eval
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 21.3% con 40t y 60 eval
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 17.1% con 50t y 50 eval
% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 14.23% con 70t y 30 eval

% Con sequencias de igual tamaño (sesiones con un número de accesos > 5 ) llego a una accurracy del 18.3% con 90t y 10 eval



% En donde mayor se tienen problemas para las predicciones son en los nodos hojas y nodos intermedios 
% los cuales tienen mas de 3 hijos con probabibilidades equiprobables de acierto


% Los otros casos donde se generan una gran cantidad de errores en las predicciones son secuencias de acceso que son equivalentes a 1 acceso.
% Dado que no hay un historial de la sesión este cae un random dentro de un subconjunto de posibles accesos equiprobables.


%%%%%%%%
%%%%%%%%

Hacemos un experimento con datos totalmente aleatorios de una muestra de 100 acceso
aparte de darle mas diveridad a las sessiones acotamos también a sesiones que por lo menos tengan
3 webaccess 

 

El orden de como se ingresan las sesiones afecta directamente proporcional a la construcción del modelo LZ Trie,
por lo cual es un factor ya que es como un FIFO al momento de crear, lo primero que lee es lo primero que entrena
por lo cual se debise tener un criterior para ordenar los webaccess antes de poder pasarlos al entrenador

@Discusión: Como LZ Trie, es en sí un compresor este no toma decisión, una posible mejora
sería implementar intrinsicamente un Decission tree dentro de subarboles dentro del trie para
poder elegir el más optimo acorde a los criterios historicos, asi podría darse el caso de ser
un compresor-predictor  ``menos tonto''


@Pregunta : ¿Porque se presenta el patron que a menos datos de que tenga el trie mayor es accurracy?

%%%%%%%%
%%%%%%%%






% Idea de experimento con cantidad acotada de nodos
% Idea de experimento con altura paramaetrizada del trieNode
% Gráficos de crecimiento del arbol medida en tiempo ms

%%%%%%%%
%%%%%%%%

















\vspace{2cm}

\section{Conclusiones y Contribuciones}


Aún cuando se presenta varios antecedentes, podemos decir que nuestro modelo ocupa bastante menos memoria pero esto va directamente relacionado el tamaño del trieNode de predicción generado.

%
Aun cuando el trieNode que representa totalmente el modelo de navegación del los usuarios de un sitio web, este no puede generalizar completamente el comportamiento estocástico de los usuarios y/o agentes que acceden a los recursos de MSNBC o un cualquier web en general.

Una de las mayores ventajas de nuestro modelo es que al estar embebido en un servidor de Machine Learning, cada nuevo evento que ingresa para la siguiente nueva ejecución esta estará mejor preparado, podríamos decir que la recolección de data de cada evento en particular nos ayudaría a que nuestro modelo en futuros trabajos vaya aprendiendo mas y más precisamente.


A medida que la altura del árbol va creciendo este genera mayor demora en cuanto a la creación del \emph{LZTrie}
% Pero a mayor altura hay mayor precisión.

Hemos validado que hacer un modelo de navegación de usuarios es un perfecta implementación para el uso de un árbol de la familia de Lempel \& Ziv.

A nuestro modelo le es afectado secuencias de menor tamaño, por lo cual se debe trabajar para hacer un aprendizaje de que momento omitirlo o no. Estas sesiones de bajo número de secuencias genera un bajo porcentaje de accurracy. 


% Cual es el minimo entrenamiento para lograr a Predecir ?

Hemos presentado un modelo liviano el cual puede ser utilizado para predecir secuencias de webaccess en demanda.






%In this paper we studied the empirical performance of a number of prominent prediction algorithms. We focused on prediction settings that are more closely related to those required

%%%%%%%%% On Prediction Using Variable Order Markov Models

%$%%%%%%%%
%%n this paper we studied the empirical performance of a number of prominent prediction algorithms. We focused on prediction settings that are more closely related to those required


%On Prediction Using Variable Order Markov Models
%by machine learning practitioners dealing with discrete sequences.
%However, somewhat surprisingly, the best predictor under the log-loss is not the best classifier. On the contrary, the consistently best protein classifier is based on the mediocre lz-ms predictor! This algo- rithm is a simple modification of the well-known Lempel-Ziv-78 (lz78) prediction algorithm, which can capture VMMs with large contexts. The surprisingly good classification accuracy achieved by this algorithm may be of independent interest to protein analysis research and clearly deserves further investigatio 
% Genero toda las referencias para demostrar el uso de la bibliografía
% No es necesario que utilice este comando en su document
%
%Conclusion of this paper Gopalratnam Cook
%a

lz modelos eficazmente procesos secuenciales, y es extremadamente útil para la predicción de los procesos donde los eventos son dependientes de la historia evento anterior. Esto es debido a la capacidad del algoritmo para construir un modelo preciso de la fuente de los eventos que se generan, una característica heredada de su información de fondo teórico y el algoritmo de compresión de texto LZ78.
%
La eficacia del método para el aprendizaje de una medida de tiempo también se puede atribuir a hecho de que ALZ es un fuerte predictor secuencial. Los principios teóricos de sonido en el que ALZ se fundamenta también significan que ALZ es un universal Quiniela óptima, y se puede utilizar en una variedad de escenarios de predicción.
%conclusion lcoa
%

Dado que la mayoría de los predictores funcionan de 
manera offline, uno de los aporte de tener estar estructura de algoritmos como servicios es poder tener un motor de predicción en linea.





\section{Trabajos Futuro}

Esta memoria forma parte del plan de continuidad para la continuación en el postgrado de la Escuela de Ingeniería Informática y telecomunicaciones, por lo cual se desea profundizar este trabajo en las discusiones realizadas. Los temas deseados por abarcar:

\begin{itemize}

\item Crear un estudio comparativo con Modelos de Machine Learning como RNN, Redes Neuronales, Reglas de Asociación, Deep Learning y algoritmo de tipo Frequent Pattern Growth.

\item Mejorar la implementación de Lempel Ziv 78, realizado con lenguaje funcional y objetos(\emph{Scala}) y hacer una comparación de rendimientos contra implementaciones clásicas en lenguajes de más bajo nivel.(C++)


\item Crear técnicas como la usada por Claude \etal~\cite{Claude2014}, para crear representaciones eficientes en función de este modelo predictivo.


\item Investigar los factores teóricos y técnicos para poder mejorar la exactitud de la predicción.

\item Ambiciosamente a realizar un estudio comparativo para encontrar puntos en común de estas áreas de la ciencia de la computación, se desea crear un nuevo algoritmo basado en la familia de \emph{Lempel Ziv}, el cual pueda tener un complemento para la selección de sesiones antes ser ingresadas en el modelo de navegación predictivo. 


	
\end{itemize}	






 








