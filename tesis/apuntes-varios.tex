



\emph{There are a great many VMM prediction algorithms. In fact, any lossless compression algorithm can be used for prediction.}~\cite{Begleiter2004} 

\emph{ The ‘probabilistic suffix tree (PST)’ algorithm is well known in the machine learning community.}~\cite{Begleiter2004} 

\emph{lz78 and lz77 attracted enormous attention and inspired the area of lossless compression and sequence prediction.}~\cite{Begleiter2004} 


\emph{Related work
In this section we briefly discuss some results that are related to the present work. We restrict ourselves to comparative studies that include algorithms we consider here and to discussions of some recent extensions of these algorithms.}~\cite{Begleiter2004}

\emph{The problem of constructing a universal predictor for sequential prediction of arbitrary deterministic sequences was first considered in (Ref. 1),}~\cite{Gopalratnam2007}


\emph{The LZ78 data compression algorithm as suggested by Lempel and Ziv (Ref. 5), is an incremental parsing algorithm that introduces exactly such a method for gradually changing the Markov order at the appropriate rate. This algorithm has been interpreted as a universal modeling scheme that sequentially calculates empirical probabilities in each context of the data, with the added advantage that the generated probabilities reflect contexts seen from the beginning of the parsed sequence to the current symbol. The LZ code length of any individual sequence attains the Markovian empirical entropy for any finite Markov order, i.e., the LZ algorithm in effect attains the Markov entropy of any given source, thereby functioning as a Universal Predictor.}~\cite{Gopalratnam2007}


\emph{A prediction model is trained with a set of training sequences.}~\cite{Gueniche2015}

\emph{A prediction consists in predicting the next items of a sequence. This task has numerous applications such as web page prefetching, consumer product recommendation, weather forecasting and stock market prediction [1, 3, 8, 11].}~\cite{Gueniche2015}


\emph{Besides, several compression algorithms have been adapted for sequence predictions such as LZ78 [12] and Active Lezi [4]. Moreover, machine learning algorithms such as neural networks and sequential rule mining have been applied to perform sequence prediction [6, 11].}~\cite{Gueniche2015}


\emph{these models suffer from some important limitations [5]. 
First, most of them assume the Markovian hypothesis that each event solely depends on the previous events. If this hypothesis does not hold, prediction accuracy using these models can severely decrease [5, 3].

Second, all these models are built using only part of the information contained in training sequences. Thus, these models do not use all the information contained in training sequences to perform predictions, and this can severely reduce their accuracy. For instance, Markov models typically considers only the last k items of training sequences to perform a pre- diction, where k is the order of the model. One may think that a solution to this problem is to increase the order of Markov models. However, increasing the order of Markov models often induces a very high state complexity, thus making them impractical for many real-life applications [3].	
}~\cite{Gueniche2015}



\emph{Data Set
The server log files are accessed only by the administrator of the web site and are not accessible to the common internet users. Therefore, it is difficult to get recent updated log files. Therefore one of the standard data sets used in the literature is taken for experimental purpose.}~\cite{Poornalatha2012}



\emph{HMM and n-gram models seem attractive but for various technical reasons they tend to be used only with small numbers of states.}~\cite{Li2005}


\emph{
Data Compression algorithms exploit characteristics such as repeating substrings (patterns) to make the compressed data smaller than the original data. Lossless compression algorithms - as opposed to "lossy" compression algorithms - ensure that the original information can be accurately reproduced from the compressed data.}~\cite{}




\emph{The use of advanced handheld devices increases rapidly. In relation to that fact, the users expectations of what they can achieve with their devices is also increasing; the handheld devices is expected to work more or less as a replacement to their standalone computer. These facts put high demands on the mobile applications to take care of the devices limited capacity; the algorithms used must be both effective and highly adapted.}~\cite{Mans Andersson}

\emph{The Lempel-Ziv Coding is not one algorithm but actually a collection of algorithms, all based on the same technique. Basically it is a dictionary compression technique where strings are switched to their corresponding index in the dictionary. The big difference between the Lempel-Ziv methods is how the dictionary is created. }~\cite{Mans Andersson}

\emph{The LZ78 algorithm uses a more traditional dictionary where past strings are added. It begins with a dictionary containing only one string, the null string or the empty string, and an empty buffer string. For every symbol read it searches the dictionary for any occurrences of the string buffer concatenated with the new symbol. If found, the symbol is added to the buffer and the next symbol is read. If not found, the string is added to the dictionary and a token consisting of two parts is created, the number of the dictionary string corresponding to the the string buffer and the symbol which was read, e.g. (3,’e’). This token is added to the compressed data and the string is stored in the dictionary}~\cite{Mans Andersson}



\emph{}~\cite{}
\emph{}~\cite{}
\emph{}~\cite{}
\emph{}~\cite{}
\emph{}~\cite{}
\emph{}~\cite{}





Resumen
En esta tesis se propone un compresor sin p ́erdida de ima ́genes satelitales multiespectrales LANDSAT. Estas ima ́genes se presentan en varias bandas, y los compresores sin p ́erdida actuales no hacen uso de la correlacio ́n presente entre ellas. El compresor propuesto se basa en la transformada wavelet y en la prediccio ́n de sus coeficientes, obtenida mediante una combinacio ́n lineal de otros coeficientes ya codificados (los cuales pueden pertenecer incluso a otra banda de la imagen). Un posterior afinamiento de la prediccio ́n es realizado discriminando entre distintas clases de terrenos presentes. Las diferencias entre la predicci ́on y el coeficiente wavelet se codifican con un codificador aritm ́etico. El proceso es realizado por bloques, codifica ́ndose cada uno de ellos en forma independiente.





SECCION COMPRESSION



% NOTAS:::::::
	 % es posible que tengamos que asumir que el origen contiene símbolos de un alfabeto y los símbolos se producen a raíz de una distribución de probabilidad específica antes de que ocurra ningún tipo de codificación.

	% Cualquier método de compresión que ver esencialmente dos tipos de trabajo: el modelado y la codificación. 

	% El modelo representa nuestro conocimiento sobre el dominio de origen y la manipulación de la redundancia de origen.

 

	%  Basado en el modelo y algunos cálculos, el codificador se utiliza para derivar un código y codificar (comprimir) la entrada. 

	%  Un codificador puede ser independiente del modelo.

	% Una estructura similar se aplica a algoritmos de decodificación. No es de nuevo un modelo y un decodificador para cualquier algoritmo de decodificación.

	%%%%%%%%%%%%%%




	%%%%%%%%%%%%%


% There is an intimate relation between prediction of discrete sequences and lossless com- pression algorithms, where, in principle, any lossless compression algorithm can be used for prediction and vice versa (see, e.g., Feder & Merhav, 1994).


% A compression approach is lossless only if it is possible to exactly reconstruct the original data from the compressed version. There is no loss of any information during the compression 1 process. [MengyiPu2006]

% Lossless compression is called reversible compression since the original data may be recovered perfectly by decompression.


% Lossless compression techniques are used when the original data of a source are so important that we cannot afford to lose any details. Examples of such source data are medical images, text and images preserved for legal reason, some computer executable files, etc.

%%%%%%%%%%G. V. CORMACK* AND R. N. S. HORSPOOL*

% All data compression methods rely on a priori assumptions about the structure of the source data. 

% A similar assumption, that there is an underlying Markov model for the data, is made in the Ziv-Lempel13.14 and the Cleary-Witten1 techniques. In fact, it is provable that Ziv- Lempel coding approaches the optimal compression factor for sufficiently long messages that are generated by a. Markov model.

% The new direction taken in our work is an algorithmic attempt to discover a Markov chain model that describes the data. If such a model can be constructed from the first part of a message, it can be used to predict forthcoming binary characters. Each state in the Markov chain supplies probabilities for the next binary character being a zero or a one. After using the probability estimate in a data coding scheme, we can use the actual message character to transfer to a new state in the Markov chain. This new state is then used to predict the next message bit, and so on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Megha Atwal, Lovnish Bansal Fast Lempel-ZIV (LZ78) Algorithm Using Codebook Hashing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Claude Shannon ignited the area of source coding with his ground breaking papers [1] in the late 1940s and early 1950s. Shannon essentially forged the theoretical background of compression using information theory for lossless compression and rate-distortion theory for lossy compression. In information theory, data compression or source coding involves encoding information using fewer bits than the original. Compression can be either lossy or lossless. Lossless compression reduces bits by removing statistical redundancy. Lossy compression reduces bits by removing unnecessary information. The process of reducing the size of a file is referred to as data compression, though formally it is called source coding.

% Compression is useful as it helps reduce resources usage, such as storage space or transmission capacity. But we must decompress the compressed data before use. This overhead

% Dictionary Coding  Static Dictionary   Semi Adaptive Dictionary  Adaptive Dictionary











 
 




 


% Un punto de vista de esta inclusión de áreas muestra que los algoritmos de compresión mapean implícitamente a las representaciones vectoriales que se usan para hacer los conjuntos de entrenamientos para la construcción de predictores, las cuales a su vez son cotas superiores. Podemos señalar que  trabajos como los de Langdon y Rissanen~\cite{RissanenLangdon1979} han sido claves para determinar la comprensibilidad de un algoritmo y la \emph{predictibilidad} que poseen los modelos de compresión. Con estos modelos se pueden realizar predicciones muy interesantes en el área de \emph{Machine Learning}.


% Desde otro punto de vista, los algoritmos de compresión mapean implícitamente strings en espacios vectoriales implícitos por features de un cierto dominio, y debido a las propiedades que usa la  compresión, podemos usarlos para los dominios de predicción y aprendizaje de ciertas características que nos entrega un entrenamiento típico de algoritmos de Machine Learning.
% colocar cita a paper de ML con LDC

% Esta idea de usar algoritmos de compresión en máquinas de aprendizaje no es nueva, pero no ha sido mayormente explorada. Los algoritmos de compresión han sido estudiados e investigados durante varios años, la motivación fundamental es poder optimizar el espacio generado por ambas áreas, para un uso eficiente o mayor almacenamiento de datos. Estos algoritmos se encuentran, sin saberlo, en nuestro día a día, desde el núcleo de un sistema operativo como Linux hasta por ejemplo los formatos \emph{zip}, \emph{rar} y \emph{7z}, también en formatos de imágenes y audios, etc. los cuales son útiles para poder optimizar una transferencia de archivos de un equipo a otro mediante Internet o simplemente comprimir datos para respaldar, por ejemplo, en dispositivos físicos. 




%Esta idea de usar algoritmos de compresión en máquinas de aprendizaje no es nueva, pero no ha sido explotada mayormente explorada.


%@TODO: THERE IS AN INTIMATE RELATION BETWEEN PREDICTION OF DISCRETE SEQUENCES AND LOSSLESS COMPRESSION
%ALGORITHMS, WHERE, IN PRINCIPLE, ANY LOSSLESS COMPRESSION ALGORITHM CAN BE USED
%FOR PREDICTION AND VICE VERSA (SEE, E.G., FEDER & MERHAV, 1994).







% The fundamental idea that data compression can be used to perform machine learning tasks has surfaced in a several areas of research, including data compression (Witten et al., 1999a; Frank et al., 2000),

% machine learning and data mining (Cilibrasi and Vitanyi, 2005; Keogh et al., 2004; Chen et al., 2004), information theory, (Li et al., 2004), bioinformatics (Chen et al., 1999; Hagenauer et al., 2004), spam filtering (Bratko and Filipic, 2005), and even physics (Benedetto et al., 2002). The principle at work is that if strings x and y compress more effectively together than they do apart, then they must share similar information.




% @TODO: 
% - hablar mas de que LZ78 es basado en un diccicionario.


%   Consider the sequence of input symbols xn = “aaababbbbbaabccddcbaaaa”. An LZ78 parsing of this string of input symbols would yield the following set of phrases: “a,aa,b,ab,bb,bba,abc,c,d,dc,ba,aaa”. As described above, this algorithm maintains statistics for all contexts seen within the phrases wi . For example, the context ‘a’ occurs 5 times (at the beginning of the phrases “a, aa, ab, abc, aaa”), the context “bb” is seen 2 times (“bb,bba”), etc. These context statistics are stored in a trie. (Fig. 2).


%empezar habalr sibre lempel ziv







%%%%@BEGLEITER
% Vamos Σ ser un alfabeto finito. Un alumno se da una secuencia de entrenamiento Q1n = q1q2 · · · qn, donde Σ ∈ qi y qiqi + 1 es la concatenación de qi y qi + 1. Basado en Q1n, el objetivo es aprender un modelo P que proporciona una asignación de probabilidad de cualquier resultado futuro dado algún pasado. En concreto, para cualquier "contexto" s ∈ Σ * y símbolo σ ∈ Σ el alumno debe generar una distribución de probabilidad condicional P (σ | s).
% Predicción del rendimiento se mide a través de la media de pérdida de log-l (P, XT1) de P (· | ·), con respecto a una secuencia de prueba XT1 = x1 ··· xT,


%%%%%%%Gueniche_Fournier-Viger_Raman_Tseng

%Therefore, an important research problem is to propose strategies to reduce the size and prediction time of CPT. Reducing the spatial complexity is a very challenging task. An effective compression strategy should provide a huge spatial gain while providing a minimum overhead in terms of training time and prediction time







%%%%%Moghaddam_Kabir
%These algorithms are compression algorithms and also are used in sequence mining. We use these algorithms for modeling the user navigation history.
%%%%%%%%























%%%%%%% \cite{Moghaddam2009}
%LZ78 basically are lossless data compression algorithms with good functionality. The most important part of these algorithms is the dictionary construction algorithm that we use it for creating the prediction model.


%%%%%%% \cite{Moghaddam2009}
%LZ78 and LZW algorithms. These algorithms are compression algorithms and also are used in sequence mining.
%We use these algorithms for modeling the user navigation history.
%%%%%%%%% 


%LZW compression has its roots in the work of Jacob Ziv and Abraham Lempel. In 1977, they published a paper on "sliding-window" compression, and followed it with another paper in 1978 on "dictionary" based compression. These algorithms were named LZ77 and LZ78, respectively. Then in 1984, Terry Welch made a modification to LZ78 which became very popular and was dubbed LZW (guess why). The LZW algorithm is what we are going to talk about here.


%%%%%%% \cite{Begleiter2004}
%El algoritmo LZ78 es uno de los algoritmos de compresión sin pérdidas más populares (Ziv y Lem- pel, 1978). Se utiliza como la base de la utilidad de Unix comprimir y otras utilidades de archivado populares para PC. También cuenta con garantías de rendimiento dentro de varios modelos de análisis. Este algoritmo (junto con el método de compresión LZ77) atrajo una enorme atención e inspiró el área de compresión sin pérdidas y la secuencia de predicción.
%El componente de predicción de este algoritmo se discutió por primera Langdon (1983) y Rissanen (1983). La presentación de este algoritmo se simplifica después del algoritmo de compresión LZ78 conocido, que funciona de la siguiente manera, se entiende. Dada una secuencia de Q1n ∈ Σn, LZ78 incrementalmente analiza Q1n en 'frases' adyacentes que no se solapan, que se recogen en una frase "diccionario". El algoritmo comienza con un diccionario que contiene el ǫ frase vacía. En cada paso del algoritmo analiza una nueva frase, que es la frase más corta que todavía no está en el diccionario. Claramente, la frase recién analizada s 'se extiende una ya existente

%frase diccionario por un símbolo; es decir, s '= sσ, donde s ya está en el diccionario
%(s puede ser la frase vacía). Para la compresión, el algoritmo codifica el índice de s '
%(entre todas las frases analizadas), seguido de un código fijo para σ. Tenga en cuenta que los problemas de codificación no lo hará
%nos concierne en este trabajo. También observe que LZ78 comprime secuencias sin explícita
%estimaciones probabilísticas. He aquí un ejemplo de este análisis LZ78: si = Q11 abracadabra, 1
%a continuación, las frases son analizados se a | b | r | ac | anuncio | ab | ra. Observe que el vacío secuencia ǫ siempre está en el diccionario y se omite en nuestras discusiones.
%Un algoritmo de predicción basado en LZ78 fue propuesto por Langdon (1983) y Rissanen (1983). Se describen por separado el aprendizaje y la predicción phases.6 Por simplicidad primera discutimos el caso binario, donde Σ = {0, 1}, pero el algoritmo se puede extender naturalmente a alfabetos de cualquier tamaño (y en los experimentos discutidos a continuación hacemos uso de la algoritmo multi-alfabeto). En la fase de aprendizaje el algoritmo construye a partir de la secuencia de entrenamiento Q1n un árbol binario (trie) que registra las frases analizados se (como se mencionó anteriormente). En el árbol también mantenemos contadores que mantienen estadísticas de Q1n. El árbol inicial contiene una raíz y dos (izquierda y derecha) se va. El hijo izquierdo de un nodo corresponde a un análisis de '0' y el hijo derecho corresponde a un análisis de '1'. Cada nodo mantiene un contador. El contador en una hoja siempre se establece en 1. El contador en un nodo interno se mantiene siempre por lo que es igual a la suma de sus izquierdo y derecho contadores niño. Dada una frase recién analizada s ', empezamos en la raíz y recorrer el árbol de acuerdo a s' (claramente el árbol contiene una ruta correspondiente, que termina en una hoja). Al llegar a una hoja, el árbol se expande haciendo que esta hoja de un nodo interno y la adición de dos foliares hijos a este nuevo nodo interno. Los contadores lo largo de la ruta a la raíz se actualizan en consecuencia.
%Para calcular la estimación de P (σ | s) se parte de la raíz y recorrer el árbol de acuerdo con s. Si llegamos a una hoja antes de "consumir" s seguimos este recorrido desde la raíz, etc. Al finalizar este recorrido (en algún nodo interno, o una hoja) la predicción para = S '0' es el '0' (izquierda ) contador dividido por la suma de '0' y contadores '1' en ese nodo, etc.
%Para alfabetos más grandes, el algoritmo se extiende, naturalmente, de tal manera que las frases son
%almacenada en un árbol de múltiples vías y cada nodo interno ha exactamente k = | Σ | niños. En adición,
%cada nodo tiene k contadores, uno para cada símbolo sea posible. En la Figura 1 que representan la resultante
%árbol de la secuencia de entrenamiento q11 = abracadabra y calcular la probabilidad P (b | ab), 1
%suponiendo Σ = {a, b, c, d, r}.
%Varios garantías de rendimiento fueron probados para la compresión LZ78 (y predicción)
%algoritmo. Dentro de un entorno probabilístico (véase la sección 2), cuando la fuente desconocida es Markov estacionario y ergódico de orden finito, la redundancia se acotado superiormente por (1 / ln n), donde n es la longitud de la secuencia de entrenamiento (Savari, 1997). Por lo tanto, el algoritmo LZ78 es un algoritmo de predicción universal con respecto a la gran clase de fuentes estacionarias y ergódicos de Markov de orden finito.

%%%%%%%%%




%%%%%%% \cite{Moghaddam2009}
%Usar el ejemplo de construccion del arbol de LZ para explicar en la tesis





% seria bonito dejar una explciacion de como se construye el tree



% Begleiter, El-Yaniv y YonaBegleiter, El-Yaniv y Yona
%El algoritmo LZ78 es uno de los algoritmos de compresión sin pérdidas más populares (Ziv y Lem- pel, 1978). Se utiliza como la base de la utilidad de Unix comprimir y otras utilidades de archivado populares para PC. También cuenta con garantías de rendimiento dentro de varios modelos de análisis. Este algoritmo (junto con el método de compresión LZ77) atrajo una enorme atención e inspiró el área de compresión sin pérdidas y la secuencia de predicción.
% El componente de predicción de este algoritmo se discutió por primera Langdon (1983) y Rissanen (1983). La presentación de este algoritmo se simplifica después del algoritmo de compresión LZ78 conocido, que funciona de la siguiente manera, se entiende. Dada una secuencia de Q1n ∈ Σn, LZ78 incrementalmente analiza Q1n en 'frases' adyacentes que no se solapan, que se recogen en una frase "diccionario". El algoritmo comienza con un diccionario que contiene el ǫ frase vacía. En cada paso del algoritmo analiza una nueva frase, que es la frase más corta que todavía no está en el diccionario. Claramente, la frase recién analizada s 'se extiende una ya existente
% 5. No se incluyeron en el presente trabajo el algoritmo de predicción que se puede derivar del algoritmo de compresión bzip más reciente (ver http://www.digistar.com/bzip2), que se basa en el éxito de Burrows-Wheeler Transform ( Burrows y Wheeler, 1994; Manzini, 2001).
% 
% frase diccionario por un símbolo; es decir, s '= sσ, donde s ya está en el diccionario
% (s puede ser la frase vacía). Para la compresión, el algoritmo codifica el índice de s '
% (entre todas las frases analizadas), seguido de un código fijo para σ. Tenga en cuenta que los problemas de codificación no lo hará
% nos concierne en este trabajo. También observe que LZ78 comprime secuencias sin explícita
% estimaciones probabilísticas. He aquí un ejemplo de este análisis LZ78: si = Q11 abracadabra, 1
% a continuación, las frases son analizados se a | b | r | ac | anuncio | ab | ra. Observe que el vacío secuencia ǫ siempre está en el diccionario y se omite en nuestras discusiones.
% Un algoritmo de predicción basado en LZ78 fue propuesto por Langdon (1983) y Rissanen (1983). Se describen por separado el aprendizaje y la predicción phases.6 Por simplicidad primera discutimos el caso binario, donde Σ = {0, 1}, pero el algoritmo se puede extender naturalmente a alfabetos de cualquier tamaño (y en los experimentos discutidos a continuación hacemos uso de la algoritmo multi-alfabeto). En la fase de aprendizaje el algoritmo construye a partir de la secuencia de entrenamiento Q1n un árbol binario (trie) que registra las frases analizados se (como se mencionó anteriormente). En el árbol también mantenemos contadores que mantienen estadísticas de Q1n. El árbol inicial contiene una raíz y dos (izquierda y derecha) se va. El hijo izquierdo de un nodo corresponde a un análisis de '0' y el hijo derecho corresponde a un análisis de '1'. Cada nodo mantiene un contador. El contador en una hoja siempre se establece en 1. El contador en un nodo interno se mantiene siempre por lo que es igual a la suma de sus izquierdo y derecho contadores niño. Dada una frase recién analizada s ', empezamos en la raíz y recorrer el árbol de acuerdo a s' (claramente el árbol contiene una ruta correspondiente, que termina en una hoja). Al llegar a una hoja, el árbol se expande haciendo que esta hoja de un nodo interno y la adición de dos foliares hijos a este nuevo nodo interno. Los contadores lo largo de la ruta a la raíz se actualizan en consecuencia.
% Para calcular la estimación de P (σ | s) se parte de la raíz y recorrer el árbol de acuerdo con s. Si llegamos a una hoja antes de "consumir" s seguimos este recorrido desde la raíz, etc. Al finalizar este recorrido (en algún nodo interno, o una hoja) la predicción para = S '0' es el '0' (izquierda ) contador dividido por la suma de '0' y contadores '1' en ese nodo, etc.
% Para alfabetos más grandes, el algoritmo se extiende, naturalmente, de tal manera que las frases son
% almacenada en un árbol de múltiples vías y cada nodo interno ha exactamente k = | Σ | niños. En adición,
% cada nodo tiene k contadores, uno para cada símbolo sea posible. En la Figura 1 que representan la resultante
% árbol de la secuencia de entrenamiento q11 = abracadabra y calcular la probabilidad P (b | ab), 1
% suponiendo Σ = {a, b, c, d, r}.
% Varios garantías de rendimiento fueron probados para la compresión LZ78 (y predicción)
% algoritmo. Dentro de un entorno probabilístico (véase la sección 2), cuando la fuente desconocida es Markov estacionario y ergódico de orden finito, la redundancia se acotado superiormente por (1 / ln n), donde n es la longitud de la secuencia de entrenamiento (Savari, 1997). Por lo tanto, el algoritmo LZ78 es un algoritmo de predicción universal con respecto a la gran clase de fuentes estacionarias y ergódicos de Markov de orden finito.