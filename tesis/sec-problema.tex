

\section{Definición del problema}


El problema de realizar modelos predictivos que minimicen su conjunto de entrenamientos, ha surgido hace años y diversos investigadores han trabajado con distintos enfoques. Los principales Rissanen\cite{Rissanen1983} y Langdom\cite{Langdon1983} en los laboratorios \emph{Bell}, al realizar pruebas y experimentar con un \emph{robot} que tiraba una moneda compitiendo con humano, aquel robot realizaba todos los cálculos {markovianos} y las probabilidades condicionales para que cierto evento ocurra, a diferencia del sujeto que sólo estaba esperando un resultado aleatorio, la diferencia se marco en los costos de tiempo y de computo que se realizaron, por un parte la demora del \emph{robot} haciendo sus cálculos no mejoraba a la probabilidad aleatoria con que la persona a que enfrentaba 

Predecir no es trivial y requiere de gran cantidad información para poder realizar un modelo que logre abarcar varios escenarios reales, pero sí podemos llegar a acercarnos y minimizar el error  estaremos más cerca a predicciones exactas. Sin embargo, dos áreas han tratado de resolver el problema;  \emph{Lossless Data Compression}(\texttt{LDC}) y \emph{Machine Learning} de manera separada. Por parte de \texttt{LDC} los mayores problemas son que los algoritmos predictivos  funcionan totalmente desconectados de la fuente de datos de entrada, esto implica que la validez del modelo solo es factible cuando esta realizando predicciones sin usuarios concurrentes o como se ha señalado anteriormente con un modelo con una componente \emph{offline}, lo que inhabilita rápidamente al modelo y en general no dan un resultado inmediato, en cambios en el esquema de \emph{Machine Learning} debemos crear un modelo para entrenar y luego poder generar una función predictiva a lo cual se le suma un gran cantidad de datos para lograr un buen entrenamiento que produce un modelo bastante pesado para poder funcionar como modelo predictivo \emph{online}. 

Nuestro problema se acota a resolver predicciones secuenciales discretas con un modelo predictivo generado por un algoritmo de compresión, sobre un conjunto de datos discretos con data sintética y real de \emph{MSNBC}\cite{Claude2014}. Teniendo en funcionamiento un modelo de componentes híbrido juntando algoritmos y arquitectura de cada área para disponerlo como un servicio inmediato, es decir crear la componente \emph{online} del modelo predictivo con ayuda de \emph{Lossless Data Compression} y el famoso algoritmo \texttt{LZ78}; dando una predictibilidad inmediata que hoy en la industria es necesaria para usar los datos recolectados y entregar nuevos enfoques a las decisiones basadas en esta perspectiva predictiva, como también dando un avance en el análisis de datos predictivos. Usando un algoritmo de tipo \texttt{LDC} reemplazando en el proceso de aprendizaje de una arquitectura de  servicios  \emph{Machine Learning} podemos alcanzar a una buena solución o a lo menos estar sobre el promedio aleatorio de ocurrencia de eventos.

