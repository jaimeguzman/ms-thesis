

\section{Definición del problema}


El problema de realizar modelos predictivo que minimicen su conjunto de entrenamientos, ha surgido hace años y diversos investigadores han trabajado con distintos enfoques. Rissman \cite{Rissanen1983} y Langdom \cite{Langdon1983} en los laboratorios {Bell} al realizar pruebas y experimentar con un robot que tiraba una moneda compitiendo con humano, el robot realizaba todos los cálculos {markovianos} y cálculos de las probabilidades condicionales para que cierto evento ocurra, a diferencia del sujeto que sólo estaba esperando un resultado.

Predecir no es trivial y requiere de gran cantidad información para poder realizar un modelo exacto, pero sí podemos llegar a acercarnos y minimizar el error de equivocarnos estaremos más cerca a predicciones exactas. Sin embargo, dos áreas han tratado de resolver el problema; {LDC} y {Machine Learning} de manera separada. Por parte de {LDC} los mayores problemas son que los algoritmos predictores funcionan totalmente desconectados de la fuente de datos, lo que implica que la validez del modelo solo es factible cuando esta realizando predicciones sin usuarios concurrentes, lo que inhabilita rápidamente al modelo y en general no dan un resultado inmediato, en cambios en el área de Machine Learning debemos crear un modelo para entrenar y luego poder generar una función predictiva a lo cual se le suma un gran cantidad de datos para lograr un buen entrenamiento que produce un modelo bastante pesado para poder funcionar como predictor \emph{online}. 

Acotaremos nuestro problema a resolver esto con un modelo predictivo secuencial y set de datos discretos. Teniendo un modelo híbrido juntando los patrones de cada área y disponerlo como un servicio inmediato; dando una predictibilidad inmediata que hoy en la industria es necesaria para poder hacer útiles estos algoritmo y dar un valor a los avances. Usando un algoritmo de \emph{Lossless compression algorithm} y corriendo este modelo en una arquitectura de un servidor de \emph{Machine learning} podemos llegar a una solución óptima.

