En \machinelearning se realizan tareas de clasificación, agrupamiento o regresión, como ya hemos introducido. Uno de los modelos que se puede usar son modelos que usan probabilidades, es decir podemos realizar un entrenamiento con una distribución de probabilidades sobre una secuencia de datos. El concepto de modelo de Markov, se basa en la idea anterior, pero no solo se lograr realizar lo anteriormente señado, existe un universo de modelos de markov como \emph{Procesos de deciones de Markov}, \emph{Markov discreto}, \emph{Cadenas de Markov de Monte Carlo para redes Bayesianas}, y  \emph{Modelo oculto de Markov}.

El \emph{Modelo oculto de Markov} (\HMM) ha sido usado tanto en el reconocimiento de voz, traducción de idiomas, clasificación de texto, etiquetado de documentos y compresión. \HMM ha sido una gran herramienta para algoritmo de \machinelearning, se puede ver mas detalle de los trabajos y soluciones que ofrece en \emph{Rabiner}~\cite{}.





\subsubsection{La propiedad Markov}



Las propiedades de Markov es una característica del proceso estocástico, donde la distribución de probabilidades condicionales de un estado a otro depende directamente del estado actual y no de los estado en que alguna vez estuvo, en el pasado. Dado lo anterior se entiende que los estados se producen en un tiempo discret, y la propiedad de Markov es conocida como \emph{cadena de Markov discreta}. Aunque elos procesos de Markov se puede aplicar como ensayo y error en varias aplicaciones, su usabilidad es limita a la resolución de problemas para los cuales las observaciones no dependen de los estados ocultos. Los Modelos ocultos de Markov son una técnica comúnmente aplicado a afrontar este reto.






\subsubsection{Textos por revisar}

% The first-order discrete Markov chain

% Este trabajo se ha centrado en la generación de un modelo predictivo de secuencias discretas sobre un alfabeto finito, con las definiciones anteriores podemos profundizando como usar técnicas de Machine Learning, que nos permitan avanzar y estar en la búsqueda de nuevas comparaciones.

\begin{lstlisting}

El aprendizaje sobre datos secuenciales, como también el reconocimiento de patrones sigue siendo una de los desafíos del área de \machinelearning.
La literatura en estos temas es extensa y se ofrecen muchos acercamientos para análisis y predicción sobre secuencias en un alfabeto finito.

Una de las técnicas mas usadas son basadas sobre los \HMM, siglas en ingles de \hiddenmarkovmodels (Cadenas ocultas de Markov) CITA A RABINEER 1989 . Los \HMM nos ofrecen un estructura flexible que puede modelar distinto orígenes de datos secuenciales. Sin embargo, trabajar con los \HMM requieren una basta compresión en el dominio de problema, para poder modelar todas sus posibles restricciones.


Existen muchos problemas en que el factor de la secuencialidad de los datos se convierten en un principal actor. Hemos estado atacando un escenario en que la ocurrencia de los datos, sin ser afectos al tiempo, el orden que van ocurriendo generan puntos a desarrollar. También dado a la flexibilidad proporcionada, un entrenamiento exitoso requiere un gran conjunto de datos para ser entrenado.

 



Si diéramos una introducción al modelamiento secuencial, es necesario introducir modelos o los efectos probabilistas


De alguna manera hay que mencionar los VMM o Modelos de markov variables, son la base de los algoritmos de predicción usando LZ78



Aquí también se debe dar una intro pequeña a que se utilizará matlab, una de las validaciones que se espera es hacer correr el modelo de LZ para compararlo con los resultados típicos que tienen el LZ.

\end{lstlisting}