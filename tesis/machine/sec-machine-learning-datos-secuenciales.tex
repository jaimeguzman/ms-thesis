%%%%%%%  
%%%%%%%  
%%%%%%%  

En \machinelearning se realizan tareas de clasificación, agrupamiento o regresión. Una de las herramientas que se puede usar para estos modelos son las que usan probabilidades, es decir, podemos realizar un entrenamiento con una distribución de probabilidades sobre una secuencia de datos. El concepto de modelo de Markov, se basa en la idea anterior, pero no solamente se puede hacer lo anterior, existe un universo de modelos de Markov como \emph{Procesos de deciones de Markov}, \emph{Markov discreto}, \emph{Cadenas de Markov de Monte Carlo para redes Bayesianas}, y  \emph{Modelo oculto de Markov}, estudiaremos este último.

El \emph{Modelo oculto de Markov} (\HMM) ha sido usado tanto en el reconocimiento de voz, traducción de idiomas, clasificación de texto, etiquetado de documentos y compresión \MLkhanna. \HMM ha sido una gran herramienta para algoritmos de \machinelearning, se puede ver mas detalle de los trabajos y soluciones a problemas que ofrece \emph{Rabiner} en~\cite{Rabiner1990}.





\subsection{La propiedad de Markov}



Las propiedad de Markov es una característica de un proceso estocástico, donde la distribución de probabilidades condicionales de un estado a otro depende directamente del estado actual y no de los estado en que alguna vez estuvo, en el pasado. Sea $i$ un estado cualquiera de la secuencia $X$ de una cadena de Markov.  El estado $i+1$, solo depende del estado $i$ del sistema y no de la evolución del anterior.


Lo anterior se extiende a que los estados se producen en un tiempo discreto, y la propiedad de Markov es conocida como \emph{cadena de Markov discreta}. Aunque los procesos de Markov se puede aplicar como ensayo y error en varias aplicaciones, su usabilidad es limitada a la resolución de problemas para los cuales las observaciones no dependen de los estados ocultos. Los Modelos ocultos de Markov son una técnica comúnmente aplicado a afrontar este problema.






%\subsubsection{Textos por revisar}

% REVISAR :
% The first-order discrete Markov chain
% Este trabajo se ha centrado en la generación de un modelo predictivo de secuencias discretas sobre un alfabeto finito, con las definiciones anteriores podemos profundizando como usar técnicas de Machine Learning, que nos permitan avanzar y estar en la búsqueda de nuevas comparaciones.

% \begin{lstlisting}

% El aprendizaje sobre datos secuenciales, como también el reconocimiento de patrones sigue siendo una de los desafíos del área de \machinelearning.
% La literatura en estos temas es extensa y se ofrecen muchos acercamientos para análisis y predicción sobre secuencias en un alfabeto finito.

% Una de las técnicas mas usadas son basadas sobre los \HMM, siglas en ingles de \hiddenmarkovmodels (Cadenas ocultas de Markov) CITA A RABINEER 1989 . Los \HMM nos ofrecen un estructura flexible que puede modelar distinto orígenes de datos secuenciales. Sin embargo, trabajar con los \HMM requieren una basta compresión en el dominio de problema, para poder modelar todas sus posibles restricciones.


% Existen muchos problemas en que el factor de la secuencialidad de los datos se convierten en un principal actor. Hemos estado atacando un escenario en que la ocurrencia de los datos, sin ser afectos al tiempo, el orden que van ocurriendo generan puntos a desarrollar. También dado a la flexibilidad proporcionada, un entrenamiento exitoso requiere un gran conjunto de datos para ser entrenado.


% De alguna manera hay que mencionar los VMM o Modelos de markov variables, son la base de los algoritmos de predicción usando LZ78



% Aquí también se debe dar una intro pequeña a que se utilizará matlab, una de las validaciones que se espera es hacer correr el modelo de LZ para compararlo con los resultados típicos que tienen el LZ.

% \end{lstlisting}